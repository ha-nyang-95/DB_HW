# 뉴스 크롤링 및 분석

✅ 뉴스 크롤링 → 키워드/임베딩 분석 → PostgreSQL 저장 자동화 코드 (전체 주석 포함 설명)
==========================================================

```python
# 필요한 외부 라이브러리들을 import 합니다.
import feedparser  # RSS 피드를 읽기 위한 라이브러리
import requests  # 웹 요청을 보내는 라이브러리
import os  # OS 환경 설정 관련 라이브러리
from bs4 import BeautifulSoup  # HTML 문서를 파싱하기 위한 라이브러리
import psycopg2  # PostgreSQL에 연결하기 위한 라이브러리
import json  # 데이터를 JSON 형식으로 변환하기 위한 라이브러리
from datetime import datetime  # 날짜와 시간 관련 기능을 위한 모듈
from dotenv import load_dotenv  # .env 파일에서 환경변수를 불러오기 위한 라이브러리
from sklearn.feature_extraction.text import TfidfVectorizer  # 텍스트에서 키워드 추출을 위한 TF-IDF 벡터라이저
from sentence_transformers import SentenceTransformer  # Ko-SBERT 모델을 통해 임베딩 추출

# .env 파일에 있는 환경 변수(DB ID, PW 등)를 불러옵니다.
load_dotenv()

# PostgreSQL 데이터베이스 연결 정보를 환경변수로부터 읽어옵니다.
DB_CONFIG = {
    "dbname": "news",  # 데이터베이스 이름
    "user": os.getenv("DB_USERNAME"),  # .env에서 불러온 사용자명
    "password": os.getenv("DB_PASSWORD"),  # 비밀번호
    "host": "localhost",  # 호스트 주소 (보통 로컬이면 localhost)
    "port": 5432  # PostgreSQL 기본 포트 번호
}

# 수집할 RSS 피드 주소 (매일경제 사회면 기사)
RSS_FEED_URL = "https://www.mk.co.kr/rss/30100041/"

# 문장을 벡터로 바꿔주는 사전학습 모델 로딩 (Ko-SBERT)
embedding_model = SentenceTransformer("jhgan/ko-sbert-nli")

# 🔍 뉴스 본문과 기자 이름을 해당 기사 링크에서 크롤링하는 함수
def extract_article_text(url):
    try:
        # 기사 페이지를 요청
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")

        content = ""  # 본문 초기화

        # 일반적으로 뉴스 본문이 들어있는 div class 선택
        wrapper = soup.find("div", class_="news_cnt_detail_wrap")
        if wrapper:
            paragraphs = wrapper.find_all("p")  # 여러 <p> 태그로 구성된 본문 추출
            if paragraphs:
                # 줄바꿈으로 이어붙인 본문 텍스트
                content = "\n\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        else:
            # 위 방식이 안 먹히는 예외적인 경우 figcaption도 시도
            figcaption = soup.select_one("figure figcaption")
            if figcaption:
                content = figcaption.get_text(strip=True)

        # 본문이 아예 없는 경우
        if not content:
            content = "[본문 없음]"

        # 기자 이름 추출 (여러 명일 수 있음)
        author_tags = soup.select("dl.author > a")
        writers = [tag.get_text(strip=True) for tag in author_tags]
        writer = ", ".join(writers) if writers else "알 수 없음"

        return content, writer

    except Exception as e:
        print(f"[본문/기자 크롤링 오류] {url} → {e}")
        return "[본문 없음]", "알 수 없음"

# 🧠 TF-IDF 기반 키워드 추출 함수
def extract_keywords(text, top_n=5):
    try:
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        tfidf_matrix = vectorizer.fit_transform([text])  # 문서 하나로 학습
        feature_names = vectorizer.get_feature_names_out()  # 단어 목록
        scores = tfidf_matrix.toarray()[0]  # 각 단어의 TF-IDF 점수
        keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:top_n]  # 높은 순
        return [word for word, _ in keywords]  # 키워드만 추출
    except Exception as e:
        print(f"[키워드 추출 오류] {e}")
        return []

# 🧬 문장 임베딩 벡터를 생성해주는 함수
def generate_embedding(text):
    try:
        return embedding_model.encode(text).tolist()  # numpy → list로 변환
    except Exception as e:
        print(f"[임베딩 오류] {e}")
        return None

# 🗂 하나의 뉴스 기사를 PostgreSQL에 저장하는 함수
def save_article_to_db(title, writer, pub_date, category, content, link):
    try:
        # DB 연결
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        # 중복 여부 확인 (이미 같은 URL이 저장되어 있으면 패스)
        cur.execute("SELECT 1 FROM news_article WHERE url = %s", (link,))
        if cur.fetchone():
            print(f"[중복] 이미 존재: {title}")
        else:
            # 키워드와 임베딩 생성
            keywords = extract_keywords(content)
            embedding = generate_embedding(content)

            # 데이터 삽입
            cur.execute(
                """
                INSERT INTO news_article 
                (title, writer, write_date, category, content, url, keywords, embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    title,
                    writer,
                    pub_date,
                    category,
                    content,
                    link,
                    json.dumps(keywords, ensure_ascii=False),  # 키워드 리스트를 JSON 형식으로 저장
                    embedding  # KoSBERT 임베딩 벡터
                )
            )
            conn.commit()
            print(f"[저장 완료] {title}")

        # 연결 종료
        cur.close()
        conn.close()

    except Exception as e:
        print(f"[DB 저장 오류] {title} → {e}")

# 🔁 전체 뉴스 수집 → 분석 → 저장 과정을 실행하는 메인 함수
def main():
    print("📰 RSS 피드 수집 시작")
    feed = feedparser.parse(RSS_FEED_URL)  # RSS 피드 파싱

    for entry in feed.entries:
        title = entry.title  # 기사 제목
        link = entry.link  # 기사 링크
        pub_date = entry.get('published_parsed')
        pub_date = datetime(*pub_date[:6]) if pub_date else datetime.now()  # 작성일자
        category = entry.get("category", "기타")  # 카테고리 (없으면 '기타')

        # 기사 본문과 기자명 가져오기
        content, writer = extract_article_text(link)
        if content and content != "[본문 없음]":
            save_article_to_db(title, writer, pub_date, category, content, link)

# 이 파일을 직접 실행했을 때만 main()을 수행
if __name__ == "__main__":
    main()
```

* * *

🧠 추가 개념 설명 정리
--------------

| 개념 | 설명 |
| --- | --- |
| **RSS 피드** | 뉴스나 블로그 등의 업데이트 정보를 자동으로 받아올 수 있도록 만든 XML 형식의 데이터 스트림 |
| **BeautifulSoup** | HTML 구조를 파싱해서 원하는 정보를 쉽게 추출하게 해주는 도구 |
| **TF-IDF** | 텍스트에서 중요한 단어를 추출하기 위한 통계적 방법 (빈도 기반) |
| **Ko-SBERT** | 한국어 문장을 벡터로 바꿔주는 인공지능 모델. 문장 간 의미를 비교 가능 |
| **PostgreSQL** | 오픈소스 관계형 데이터베이스 시스템. 여기선 뉴스 데이터를 저장하는 용도로 사용 |
| **.env 파일** | 비밀번호나 계정 정보 같은 민감한 값을 따로 보관하는 환경 변수 파일 |
