# 배치처리 워크플로우 파이프라인 구축
## 목표
Airflow를 활용한 스케줄링(Workflow Orchestration)과 Sprak를 이용한 배치 처리를 구현하는 것
배치가 완료된 후에는 원본 데이터(JSON 등)를 별도의 아카이빙 폴더로 이동하여, 전체 데이터 관리가 용이하도록 설계
Airflow DAG를 구성하여 배치 스케줄링(예시: 매일 새벽 1시 실행)할 수 있다.
Spark로 로컬 파일 시스템에서 전처리/분석 작업을 수행할 수 있다.
전날 기사를 집계 후, 트렌드/키워드 리포트 PDF를 자동 생성할 수 있다.
## 준비사항
### 사용 데이터
로컬 파일 시스템에 저장된 기사 데이터(예: ../realtime/*.json)
Spark가 읽어올 JSON 형식(뉴스 내용, 작성일시, 키워드 정보 등)
### 개발언어/프로그램
Python: 스크립트(배치 로직, Spark 처리 등)
Apache Spark: 데이터프레임, RDD 기반 분석 및 시각화(로컬 파일 시스템에서 데이터 로드)
Apache Airflow: 배치 스케줄링, Workflow Orchestration
Matplotlib: 리포트용 시각화(차트, 그래프)
파일 및 디렉터리 관리: Python 표준 라이브러리(os, shutil) 활용
## 구현방법
### 1) Airflow DAG 구성
(예시: 매일 새벽 1시)에 실행되는 DAG를 구성합니다.
SparkSubmitOperator로 Spark 배치 스크립트 실행(daily_report.py 등)
분석 결과물(차트, PDF 리포트)을 특정 폴더에 저장
### 2) Spark 배치 처리
전날(전일 0시~24시) 데이터만 필터링하여 키워드, 트렌드, 기사 수 등을 집계
Matplotlib 등 라이브러리로 차트 시각화 후, PDF 형태 리포트를 자동 생성
### 3) 파일 아카이빙
분석에 사용된 JSON 파일들을 작업이 끝나면 ../news_archive 디렉터리로 이동
Python의 os, shutil 라이브러리를 사용하여 이동 처리
로그 또는 Airflow 태스크를 통해 이동 성공 여부 기록
### 4) 리포트 관리
생성된 daily_report_YYYYMMDD.pdf(또는 PNG,HTML 등)은 로컬 파일 시스템에 저장
필요 시 Airflow EmailOperator를 통해 보고서를 메일로 전송하거나, 협업 툴(Slack, MatterMost 등)에 업로드
## 관통 프로젝트 가이드
### 1)pjt 내용
#### 데이터 수집
Kafka + Flink로 실시간 뉴스 데이터를 로컬 디렉토리(../realtime/*.json)에 저장
#### Spark 배치 처리
Airflow 스케줄러가 (예시: 매일 새벽 1시) 특정 시간에 Spark 스크립트를 실행
전일 데이터(전날 0시 ~24시)만 필터링하여 키워드, 트렌드, 기사 수 등을 분석
#### 리포트 생성
Matplotlib 등 라이브러리로 상위 10개 키워드, 트렌드 등 차트 시각화 -> PDF로 저장
파일명을 날짜 기준으로 생성(예시: daily_report_20250502.pdf)
#### 데이터 아카이빙
분석에 사용된 원본 JSON 파일을 ../news_archive 디렉토리로 이동
Airflow에서 테스크 순서대로 실행(분석->이동->완료)
## 요구사항
### 기본 기능
Airflow + Spark를 이용한 배치 파이프라인 구축
    매일 특정 시간(예시: 새벽 1시)에 자동 실행
    Spark로 로컬 디렉터리의 JSON 데이터를 필터링, 집계 분석
### 요청 조건
Airflow DAG를 작성하여 정해진 스케줄에 자동 실행
Spark 스크립트를 통해 전날 데이터만 집계/분석
키워드, 트렌드 통계 등 기본 로직 포함
## 결과
배치 완료 후 콘솔 or Airflow 로그에서 집계 결과 확인
Airflow UI에서 DAG 설공/실패 상태 관리

## 요구사항 2
### 원본 데이터 아카이빙 + 2차 가공
배치 처리 완료 후, 원본 파일을 ../news_archive 디렉터리로 이동
DB 적재 등 2차 가공 프로세스 확장
### 요청 조건
Python 표준 라이브러리(shutil.move)등을 활용해 파일 이동
ETL 작업 확장을 위한 구조(예: DB 적재, 피처 엔지니어링 등)
### 결과
배치 반복 시 ../realtime 디렉터리는 항상 최근 데이터만 유지
../news_archive에 과거 데이터가 쌓여 관리

## 요구사항 3
### 트렌드 분석 및 PDF 리포트 자동 생성
### 요청 조건
하루 치 데이터 집계 후, 상위 키워드 TOP 10 등 시각화
PDF 파일 내부에 차트/그래프 포함
### 결과
리포트 파일(예시: daily_report_20250502.pdf) 자동 생성
정해진 디렉터리에 PDF 결과 저장

## 산출물
### Gitlab에 올라온 baseline 코드 기반의 레포지토리 지속적 커밋
### 이후 PJT도 백엔드, 프론트엔드 구현을 제외한 내용은 해당 레포지토리에 이어 나가면서 개발