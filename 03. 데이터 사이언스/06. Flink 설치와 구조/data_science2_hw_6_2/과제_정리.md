# Flink WordCount 분석 설명

🧠 PyFlink WordCount 과제 정리 (비전공자용 완성본)
======================================

* * *

✅ 0. 목표: 이 실습은 뭘 배우는 건가요?
-------------------------

금융 뉴스 기사 데이터에서 **단어들의 출현 빈도수(WordCount)** 를 계산하는 것이 목표입니다.

*   **Flink**라는 실시간 데이터 처리 툴을 사용해서,
    
*   **CSV 파일에서 뉴스 기사 본문(news\_text 컬럼)** 을 읽고
    
*   문장을 단어로 쪼개어 가장 많이 등장한 단어들을 찾는 과제입니다.
    

> 📈 이후에는 주요 키워드 트렌드 분석이나 예측 모델의 기초로 활용될 수 있어요.

* * *

✅ 1. 환경 준비
----------

### 💻 실습 환경: WSL2 + Ubuntu + Flink 1.20.0

* * *

### 1-1. 가상환경 생성 및 실행

```bash
python3 -m venv ~/venvs/flink
source ~/venvs/flink/bin/activate
```

> `(flink)` 표시가 터미널 앞에 생기면 성공!

* * *

### 1-2. `requirements.txt` 설정

파일에 아래와 같이 작성:

```
pyflink==1.20.0
pandas
```

### 1-3. 패키지 설치

```bash
pip install -r requirements.txt
```

* * *

✅ 2. Flink 설치 및 실행
------------------

### 2-1. Flink 압축 해제

```bash
tar -xvzf flink-1.20.0-bin-scala_2.12.tgz
```

### 2-2. Flink 클러스터 실행

```bash
cd ~/usr/local/flink-1.20.0/bin
./start-cluster.sh
```

### 2-3. 실행 상태 확인

```bash
jps
```

결과 예시:

```
18073 StandaloneSessionClusterEntrypoint  ✅ ← 이게 JobManager
31194 TaskManagerRunner
```

* * *

✅ 3. WordCount 코드 작성 (`skeleton.py`)
------------------------------------

### 📌 코드 전체 (자세한 설명 포함)

```python
# 판다스는 데이터를 쉽게 다루기 위한 라이브러리예요 (CSV 파일 불러올 때 사용).
import pandas as pd

# PyFlink에서 데이터 스트림 환경을 설정할 때 사용되는 클래스입니다.
from pyflink.datastream import StreamExecutionEnvironment

# 데이터의 타입을 명시해줄 때 쓰는 도구입니다 (예: 문자열, 정수).
from pyflink.common.typeinfo import Types

# 메인 함수 정의: 프로그램의 시작 지점입니다.
def main():
    # ✅ 1. Flink 실행 환경 만들기
    # Flink를 실행할 수 있는 기본 환경을 생성합니다.
    env = StreamExecutionEnvironment.get_execution_environment()

    # 작업을 병렬로 처리할 수 있지만, 여기선 학습 목적이므로 1개로 설정합니다.
    env.set_parallelism(1)

    # ✅ 2. 데이터 불러오기 (CSV)
    # pandas를 사용해서 CSV 파일을 불러옵니다.
    df = pd.read_csv("../data/data.csv")  # 경로는 본인 폴더에 맞게 수정하세요.

    # 'news_text' 컬럼에서 결측값(NaN)을 제거한 후, 리스트로 변환합니다.
    news_texts = df["news_text"].dropna().tolist()

    # ✅ 3. 리스트 → Flink 데이터 스트림으로 변환
    text_stream = env.from_collection(news_texts, type_info=Types.STRING())

    # ✅ 4. WordCount 파이프라인 구성
    word_count = (
        text_stream

        # 문장을 단어 단위로 나누고, 각 단어에 1을 붙여 튜플로 만듭니다.
        .map(
            lambda text: [(word.lower(), 1) for word in text.split()],
            output_type=Types.LIST(
                Types.TUPLE([Types.STRING(), Types.INT()])
            )
        )

        # 리스트 안의 튜플들을 낱개로 풀어줍니다.
        .flat_map(
            lambda words: words,
            output_type=Types.TUPLE([Types.STRING(), Types.INT()])
        )

        # 같은 단어끼리 묶습니다 (key: 단어)
        .key_by(lambda x: x[0])

        # 단어별로 count를 누적합니다.
        .reduce(lambda a, b: (a[0], a[1] + b[1]))
    )

    # ✅ 5. 결과 출력
    word_count.print()

    # ✅ 6. Flink 작업 실행
    env.execute("Finance News WordCount")

# 이 파일이 직접 실행될 경우에만 main()을 호출합니다.
if __name__ == "__main__":
    main()
```

* * *

✅ 4. 실행 방법
----------

Flink 클러스터를 켜놓은 상태에서 실행:

```bash
python skeleton.py
```

> 결과는 터미널에 단어와 출현 횟수 형태로 출력됩니다.

예:

```
('ai', 12)
('cloud', 7)
('uncertainty', 5)
```

* * *

✅ 5. 클러스터 종료
------------

작업이 끝나면 아래 명령어로 Flink 클러스터를 종료하세요:

```bash
./stop-cluster.sh
```

* * *

🧠 추가 설명 요약
-----------

| 개념 | 설명 |
| --- | --- |
| `map()` | 문장을 단어 리스트로 바꾸고 각 단어에 `(word, 1)` 부여 |
| `flat_map()` | 리스트 안 튜플을 낱개로 분리 |
| `key_by()` | 같은 단어끼리 묶음 |
| `reduce()` | 단어별 count 누적 (합산) |
| `print()` | 결과를 터미널에 출력 |
| `env.execute()` | 실제로 Flink에게 “시작해!”라고 지시하는 명령 |

* * *

🎯 실습이 갖는 의미
------------

이 WordCount 실습은 단순한 예제처럼 보일 수 있지만,

*   **뉴스 키워드 트렌드 분석**
    
*   **시장 예측을 위한 전처리**
    
*   **실시간 데이터 처리 연습**
    

과 같은 실제 문제 해결에 필요한 기반 기술을 익히는 데 아주 좋은 훈련입니다.

* * *

필요하다면 다음 확장 기능도 추천드려요:

*   🔹 결과 상위 N개 단어만 정렬해서 출력하기
    
*   🔹 특수문자 제거 및 stopwords 제거
    
*   🔹 WordCloud 이미지로 시각화
    
*   🔹 결과 CSV 또는 JSON 파일로 저장
    

언제든 질문 주시면 이어서 도와드릴게요! 🙌



---
Powered by [ChatGPT Exporter](https://www.chatgptexporter.com)