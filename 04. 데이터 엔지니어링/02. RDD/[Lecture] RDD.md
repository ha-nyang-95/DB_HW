ğŸ“˜ RDD ì •ë¦¬ ë…¸íŠ¸
============
```text
[Abstract]
Apache Sparkì˜ í•µì‹¬ ì¶”ìƒí™”ì¸ RDD(Resilient Distributed Dataset)ëŠ” ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì˜ íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ëŠ” ë¶„ì‚° ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤. RDDëŠ” ë¶ˆë³€ì„±(Immutable), íƒ„ë ¥ì„±(Resilient), íƒ€ì… ì•ˆì •ì„±(Type-safe), ì§€ì—° í‰ê°€(Lazy Evaluation) ë“±ì˜ íŠ¹ì§•ì„ ê°–ì¶”ê³  ìˆìœ¼ë©°, ì •í˜• ë° ë¹„ì •í˜• ë°ì´í„°ë¥¼ ëª¨ë‘ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

RDDëŠ” ë‘ ê°€ì§€ ì£¼ìš” ì—°ì‚°ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤: ë³€í™˜(Transformations)ê³¼ ì•¡ì…˜(Actions). ë³€í™˜ ì—°ì‚°ì€ ê¸°ì¡´ RDDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•˜ë©°, ì§€ì—° í‰ê°€ë˜ì–´ ì•¡ì…˜ ì—°ì‚°ì´ í˜¸ì¶œë  ë•Œê¹Œì§€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë³€í™˜ ì—°ì‚°ìœ¼ë¡œëŠ” map(), flatMap(), filter(), groupByKey(), reduceByKey() ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì•¡ì…˜ ì—°ì‚°ì€ ë³€í™˜ ì—°ì‚°ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ì™¸ë¶€ ì €ì¥ì†Œì— ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ì£¼ìš” ì•¡ì…˜ ì—°ì‚°ìœ¼ë¡œëŠ” collect(), count(), reduce(), saveAsTextFile() ë“±ì´ ìˆìŠµë‹ˆë‹¤.â€‹

RDDëŠ” ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ì»¬ë ‰ì…˜ì„ RDDë¡œ ë³€í™˜í•˜ëŠ” parallelize() ë©”ì„œë“œ, ì™¸ë¶€ íŒŒì¼ì—ì„œ RDDë¥¼ ìƒì„±í•˜ëŠ” textFile() ë©”ì„œë“œ ë“±ì´ ìˆìœ¼ë©°, CSVë‚˜ JSON íŒŒì¼ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë³€í™˜í•œ í›„ RDDë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤.â€‹

RDDì˜ ë³€í™˜ ì—°ì‚°ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ í•„í„°ë§, ì§‘ê³„ ë“±ì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, map()ì„ ì‚¬ìš©í•˜ì—¬ ê° ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê±°ë‚˜, filter()ë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìš”ì†Œë§Œ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, groupByKey()ì™€ reduceByKey()ë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ê±°ë‚˜ ì§‘ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

ì•¡ì…˜ ì—°ì‚°ì„ í†µí•´ ë³€í™˜ëœ RDDë¥¼ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì–»ê±°ë‚˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, collect()ë¥¼ ì‚¬ìš©í•˜ì—¬ RDDì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë“œë¼ì´ë²„ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ê°€ì ¸ì˜¤ê±°ë‚˜, saveAsTextFile()ì„ ì‚¬ìš©í•˜ì—¬ RDDë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

ì´ëŸ¬í•œ RDDì˜ ê°œë…ê³¼ ì—°ì‚°ë“¤ì„ ì´í•´í•˜ê³  í™œìš©í•¨ìœ¼ë¡œì¨, Apache Sparkë¥¼ ì‚¬ìš©í•œ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```

------------------


1\. RDDì˜ ê°œë…ê³¼ íŠ¹ì§• ì´í•´
------------------

### RDDë€?

**RDD(Resilient Distributed Dataset)** ëŠ” Apache Sparkì˜ í•µì‹¬ ë°ì´í„° êµ¬ì¡°ë¡œ, í´ëŸ¬ìŠ¤í„° ë‚´ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚°ë˜ì–´ ì €ì¥ë˜ëŠ” ë¶ˆë³€(immutable)í•œ ë°ì´í„° ì»¬ë ‰ì…˜ì…ë‹ˆë‹¤. RDDëŠ” ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ë©°, ë°ì´í„° ì†ì‹¤ ì‹œ ë³µêµ¬ ê°€ëŠ¥í•œ íŠ¹ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.â€‹

### RDDì˜ íŠ¹ì§•

#### 1\. ë°ì´í„°ì˜ ì¶”ìƒí™” (Data Abstraction)

RDDëŠ” ë¶„ì‚°ëœ ë°ì´í„°ë¥¼ ì¶”ìƒí™”í•˜ì—¬, ì‚¬ìš©ìê°€ ë³µì¡í•œ ë¶„ì‚° ì²˜ë¦¬ ë¡œì§ì„ ì§ì ‘ ë‹¤ë£¨ì§€ ì•Šê³ ë„ ë°ì´í„°ë¥¼ ì¡°ì‘í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.â€‹

#### 2\. íƒ„ë ¥ì„± (Resilient) & ë¶ˆë³€ì„± (Immutable)

*   **ë¶ˆë³€ì„±**: RDDëŠ” ìƒì„± í›„ ë³€ê²½í•  ìˆ˜ ì—†ìœ¼ë©°, ë³€í™˜ ì‹œ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
*   **íƒ„ë ¥ì„±**: RDDëŠ” lineage ì •ë³´ë¥¼ í†µí•´ ë°ì´í„° ì†ì‹¤ ì‹œ ìë™ìœ¼ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹
    

#### 3\. íƒ€ì… ì•ˆì •ì„± (Type-safe)

RDDëŠ” ê°•ë ¥í•œ íƒ€ì… ê²€ì‚¬ë¥¼ ì§€ì›í•˜ì—¬, ì»´íŒŒì¼ ì‹œì ì— ì˜¤ë¥˜ë¥¼ ë°œê²¬í•  ìˆ˜ ìˆì–´ ì•ˆì •ì ì¸ ì½”ë“œ ì‘ì„±ì„ ë„ì™€ì¤ë‹ˆë‹¤.â€‹

#### 4\. ì •í˜• & ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬

RDDëŠ” í…ìŠ¤íŠ¸, JSON, CSV ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´, ì •í˜• ë° ë¹„ì •í˜• ë°ì´í„° ëª¨ë‘ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•©ë‹ˆë‹¤.    
â€‹ë¹„ì •í˜• ë°ì´í„° -> sc.textFile() í™œìš© / ì •í˜• ë°ì´í„° -> DataFrame ë˜ëŠ” RDD.map() í™œìš©

#### 5\. ì§€ì—° í‰ê°€ (Lazy Evaluation)

RDDì˜ ë³€í™˜ ì—°ì‚°ì€ ì§€ì—° í‰ê°€ë˜ì–´, ì‹¤ì œë¡œ ì•¡ì…˜ ì—°ì‚°ì´ í˜¸ì¶œë˜ê¸° ì „ê¹Œì§€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.     
ì´ë¥¼ í†µí•´ ìµœì í™”ëœ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

### Spark RDDë€?

Sparkì—ì„œ RDDëŠ” ê¸°ë³¸ ë°ì´í„° ì¶”ìƒí™”ë¡œ, ë¶„ì‚°ëœ ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.â€‹

#### ì£¼ìš” êµ¬ì„± ìš”ì†Œ

*   **ì˜ì¡´ì„± ì •ë³´ (Lineage)**: RDDê°€ ì–´ë–»ê²Œ ìƒì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ì¶”ì í•˜ëŠ” ì •ë³´ë¡œ, ì¥ì•  ë°œìƒ ì‹œ ë³µêµ¬ì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    
*   **íŒŒí‹°ì…˜ (Partition)**: RDDëŠ” ì—¬ëŸ¬ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ë‰˜ì–´ í´ëŸ¬ìŠ¤í„°ì˜ ë…¸ë“œì— ë¶„ì‚° ì €ì¥ë©ë‹ˆë‹¤.
    
*   **ì—°ì‚° í•¨ìˆ˜**: ê° íŒŒí‹°ì…˜ì— ì ìš©ë˜ëŠ” í•¨ìˆ˜ë¡œ, ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë¡œì§ì„ ì •ì˜í•©ë‹ˆë‹¤.â€‹
  
```python
from pyspark import SparkContext

sc = SparkContext("local", "LazyEvalExample")

# 1. í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© -> Transformation
rdd = sc.parallelize(["apple", "banana", "spark", "data"])

# 2. ëŒ€ë¬¸ìë¡œ ë°”ê¾¸ê¸° -> Transformation
upper_rdd = rdd.map(lambda x: x.upper())

# 3. SPARKê°€ í¬í•¨ëœ ë¬¸ìì—´ë§Œ í•„í„°ë§ -> Transformation
filtered_rdd = upper_rdd.filter(lambda x: "SPARK" in x)

# ì§€ê¸ˆê¹Œì§€ëŠ” ì•„ë¬´ê²ƒë„ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ!

# 4. ê²°ê³¼ í™•ì¸(Action)
result = filtered_rdd.collect()
```

2\. RDD ìƒì„± ë° ë³€í™˜ í•™ìŠµ
------------------

### 2.1 RDD ìƒì„±

#### 1\. ê¸°ì¡´ì˜ ë©”ëª¨ë¦¬ ë°ì´í„°ë¥¼ RDDë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•

Sparkì—ì„œëŠ” ë¡œì»¬ ì»¬ë ‰ì…˜ì„ RDDë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ `parallelize()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.â€‹[DATA & AI](https://ingu627.github.io/spark/spark_db18/?utm_source=chatgpt.com)

```python
data = [1, 2, 3, 4, 5]  # ë¡œì»¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
rdd = sc.parallelize(data)  # ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ë³‘ë ¬í™”
```

ì—¬ê¸°ì„œ `sc`ëŠ” `SparkContext`ë¥¼ ì˜ë¯¸í•˜ë©°, `parallelize()`ëŠ” ë¡œì»¬ ë°ì´í„°ë¥¼ ë¶„ì‚°ëœ RDDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.â€‹

#### 2\. ì™¸ë¶€ íŒŒì¼(í…ìŠ¤íŠ¸, CSV, JSON ë“±)ì—ì„œ RDDë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•

SparkëŠ” ë‹¤ì–‘í•œ ì™¸ë¶€ íŒŒì¼ í˜•ì‹ì—ì„œ RDDë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

*   **í…ìŠ¤íŠ¸ íŒŒì¼**: `textFile()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¼ì¸ì„ RDDì˜ ìš”ì†Œë¡œ ì½ì–´ì˜µë‹ˆë‹¤.â€‹[DATA & AI](https://ingu627.github.io/spark/spark_db18/?utm_source=chatgpt.com)
    
    ```python
    rdd = sc.textFile("path/to/file.txt")  # í…ìŠ¤íŠ¸ íŒŒì¼ì„ RDDë¡œ ì½ì–´ì˜´ (í•œ ì¤„ì”© ìš”ì†Œë¡œ êµ¬ì„±)
    ```
    
*   **CSV íŒŒì¼**: `SparkSession`ì„ ì‚¬ìš©í•˜ì—¬ DataFrameìœ¼ë¡œ ì½ì€ í›„, `rdd` ë©”ì„œë“œë¥¼ í†µí•´ RDDë¡œ ë³€í™˜í•©ë‹ˆë‹¤.â€‹[DATA & AI](https://ingu627.github.io/spark/spark_db18/?utm_source=chatgpt.com)
    
    ```python
    df = spark.read.option("header", True).csv("path/to/file.csv")  # í—¤ë”ê°€ ìˆëŠ” CSV íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
    rdd = df.rdd  # DataFrameì„ RDDë¡œ ë³€í™˜
    ```
    
*   **JSON íŒŒì¼**: CSVì™€ ìœ ì‚¬í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.â€‹
    
    ```python
    df = spark.read.json("path/to/file.json")  # JSON íŒŒì¼ì„ DataFrameìœ¼ë¡œ ì½ê¸°
    rdd = df.rdd  # DataFrameì„ RDDë¡œ ë³€í™˜
    ```
    

#### Parallelize()

`parallelize()`ëŠ” ë¡œì»¬ ì»¬ë ‰ì…˜ì„ RDDë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹

```python
data = ["apple", "banana", "cherry"]  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
rdd = sc.parallelize(data)  # ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ë³‘ë ¬í™”
```

ì˜µì…˜ìœ¼ë¡œ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹[DATA & AI](https://ingu627.github.io/spark/spark_db18/?utm_source=chatgpt.com)

```python
rdd = sc.parallelize(data, numSlices=2)  # ë°ì´í„°ë¥¼ 2ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ RDD ìƒì„±
```

#### sc.textFile()

`textFile()`ì€ ì™¸ë¶€ í…ìŠ¤íŠ¸ íŒŒì¼ì„ RDDë¡œ ì½ì–´ì˜µë‹ˆë‹¤.â€‹

```python
rdd = sc.textFile("path/to/file.txt")  # í…ìŠ¤íŠ¸ íŒŒì¼ì„ RDDë¡œ ì½ì–´ì˜´ (ê° ì¤„ì´ í•˜ë‚˜ì˜ ìš”ì†Œ)
```

ì´ ë©”ì„œë“œëŠ” HDFS, ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ íŒŒì¼ ì‹œìŠ¤í…œì„ ì§€ì›í•©ë‹ˆë‹¤.â€‹[ì œ2ì˜ ê°œë°œê³µë¶€](https://jineeblog.tistory.com/11?utm_source=chatgpt.com)

### 2.2 RDD ë³€í™˜ (Transformations)

Spark RDDì˜ ë³€í™˜ ì—°ì‚°ì€ ê¸°ì¡´ RDDë¥¼ ë³€í˜•í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì€ ì§€ì—° í‰ê°€(Lazy Evaluation) ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ì—¬, ì‹¤ì œë¡œ ì•¡ì…˜ ì—°ì‚°ì´ í˜¸ì¶œë˜ê¸° ì „ê¹Œì§€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€‹

#### MAP

`map()` í•¨ìˆ˜ëŠ” RDDì˜ ê° ìš”ì†Œì— ì£¼ì–´ì§„ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ì—°ì‚°ì€ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ìš”ì†Œ ìˆ˜ê°€ ë™ì¼í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize(range(1, 6))  # Pythonì€ range(1, 6)ìœ¼ë¡œ 1~5 ìƒì„±
mapped_rdd = rdd.map(lambda x: x * 2)
result = mapped_rdd.collect()
print(result)  # ì¶œë ¥: [2, 4, 6, 8, 10]
```

#### FLATMAP

`flatMap()` í•¨ìˆ˜ëŠ” ê° ì…ë ¥ ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ê³ , ê²°ê³¼ë¥¼ í‰íƒ„í™”í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì£¼ë¡œ ë¬¸ìì—´ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize(["Hello World", "Apache Spark"])  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ìƒì„±
words_rdd = rdd.flatMap(lambda line: line.split(" "))  # ê° ì¤„ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬
result = words_rdd.collect()
print(result)  # ì¶œë ¥: ['Hello', 'World', 'Apache', 'Spark']
```

#### FILTER

`filter()` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ìš”ì†Œë§Œì„ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize(range(1, 11))  # 1ë¶€í„° 10ê¹Œì§€ RDD ìƒì„±
even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)  # ì§ìˆ˜ë§Œ í•„í„°ë§
result = even_numbers_rdd.collect()
print(result)  # ì¶œë ¥: [2, 4, 6, 8, 10]
```

#### MAPPARTITIONS

`mapPartitions()` í•¨ìˆ˜ëŠ” ê° íŒŒí‹°ì…˜ì— í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.     
ì´ ì—°ì‚°ì€ íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ, `map()`ë³´ë‹¤ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆì§€ë§Œ, íŒŒí‹°ì…˜ í¬ê¸°ê°€ í¬ë‹¤ë©´ ì‹¤í–‰ì´ ì•ˆë  ìˆ˜ë„ ìˆë‹¤.

```python
rdd = sc.parallelize(range(1, 10), 3)  # 1ë¶€í„° 9ê¹Œì§€ ë°ì´í„°ë¥¼ 3ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ  RDD ìƒì„±
partition_mapped_rdd = rdd.mapPartitions(lambda iter: map(lambda x: x * 2, iter))  # íŒŒí‹°ì…˜ ë‹¨ìœ„ë¡œ 2ë°° ì²˜ë¦¬
result = partition_mapped_rdd.collect()
print(result)  # ì¶œë ¥: [2, 4, 6, 8, 10, 12, 14, 16, 18]
```

#### MAPPARTITIONS WITH INDEX
- ê° íŒŒí‹°ì…˜ì— ëŒ€í•´ íŒŒí‹°ì…˜ ì¸ë±ìŠ¤ì™€ í•´ë‹¹ íŒŒí‹°ì…˜ì˜ ì´í„°ë ˆì´í„°ë¥¼ ì¸ìë¡œ ë°›ì•„ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë³€í™˜ í•¨ìˆ˜ì…ë‹ˆë‹¤.
- ê²°ê³¼ëŠ” ìƒˆë¡œìš´ RDDë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.
- ì¼ë°˜ mapPartitions()ì™€ ë‹¬ë¦¬, ì–´ëŠ íŒŒí‹°ì…˜ì—ì„œ ìˆ˜í–‰ë˜ëŠ”ì§€ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
rdd = sc.parallelize(["apple", "banana", "cherry", "date", "eggfruit"], 3)  # 3ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ”

def show_partition_index(index, iterator):
    return [f"Partition {index}: {item}" for item in iterator]

partitioned_rdd = rdd.mapPartitionsWithIndex(show_partition_index)
result = partitioned_rdd.collect()

print(result)
```

#### KEYBY

`keyBy()` í•¨ìˆ˜ëŠ” RDDì˜ ê° ìš”ì†Œë¥¼ í‚¤-ê°’ ìŒìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Pair RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

```python
# ê¸€ìì˜ ê¸¸ì´ë¡œ INDEXë¥¼ ìƒì„±
rdd = sc.parallelize(["apple", "banana", "cherry"])  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ìƒì„±
keyed_rdd = rdd.keyBy(lambda word: len(word))  # ë‹¨ì–´ ê¸¸ì´ë¥¼ í‚¤ë¡œ ì§€ì •
result = keyed_rdd.collect()
print(result)  # ì¶œë ¥: [(5, 'apple'), (6, 'banana'), (6, 'cherry')]

# ê¸€ìì˜ ì•ê¸€ìë¡œ INDEXë¥¼ ìƒì„±
rdd = sc.parallelize(["apple", "banana", "cherry"])  # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ RDDë¡œ ìƒì„±
keyed_rdd = rdd.keyBy(lambda word: word[0])  # ë‹¨ì–´ ê¸¸ì´ë¥¼ í‚¤ë¡œ ì§€ì •
result = keyed_rdd.collect()
print(result)  # ì¶œë ¥: [('a', 'apple'), ('b', 'banana'), ('c', 'cherry')]
```

#### JOIN

`join()` í•¨ìˆ˜ëŠ” ë‘ ê°œì˜ RDDë¥¼ í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

```python
rdd1 = sc.parallelize([("a", 1), ("b", 2)])  # ì²« ë²ˆì§¸ (key, value) RDD
rdd2 = sc.parallelize([("a", 3), ("b", 4), ("a", 5)])  # ë‘ ë²ˆì§¸ (key, value) RDD
joined_rdd = rdd1.join(rdd2)  # í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¡°ì¸
result = joined_rdd.collect()
print(result)  # ì¶œë ¥: [('a', (1, 3)), ('b', (2, 4)), ('a', (1, 5))]
```

#### GROUPBY

`groupBy()` í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ í•¨ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize(range(1, 6))  # 1ë¶€í„° 5ê¹Œì§€ RDD ìƒì„±
grouped_rdd = rdd.groupBy(lambda x: x % 2)  # í™€ìˆ˜/ì§ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
result = {k: list(v) for k, v in grouped_rdd.collect()}
print(result)  # ì¶œë ¥: {1: [1, 3, 5], 0: [2, 4]}
```

#### GROUPBYKEY

- `groupByKey()` í•¨ìˆ˜ëŠ” í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ê·¸ë£¹í™”í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹
- `groupBy()`ë³´ë‹¤ `groupByKey()`ë¥¼ ê¶Œì¥
```python
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])  # (key, value) í˜•íƒœì˜ RDD ìƒì„±
grouped_rdd = rdd.groupByKey()  # í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ë“¤ì„ ê·¸ë£¹í™”
result = {k: list(v) for k, v in grouped_rdd.collect()}
print(result)  # ì¶œë ¥: {'a': [1, 3], 'b': [2]}
```

`groupByKey()`ëŠ” ëª¨ë“  ê°’ì„ ì…”í”Œë§í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ê°€ëŠ¥í•œ ê²½ìš° `reduceByKey()`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.â€‹

#### REDUCEBYKEY

`reduceByKey()` í•¨ìˆ˜ëŠ” í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ë³‘í•©í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])  # (key, value) í˜•íƒœì˜ RDD ìƒì„±
reduced_rdd = rdd.reduceByKey(lambda a, b: a + b)  # í‚¤ë³„ë¡œ ê°’ì„ í•©ì‚°
result = reduced_rdd.collect()
print(result)  # ì¶œë ¥: [('a', 4), ('b', 2)]
```

`reduceByKey()`ëŠ” ë¡œì»¬ì—ì„œ ë¨¼ì € ë³‘í•©ì„ ìˆ˜í–‰í•œ í›„ ì…”í”Œë§í•˜ë¯€ë¡œ, `groupByKey()`ë³´ë‹¤ íš¨ìœ¨ì ì…ë‹ˆë‹¤.â€‹


#### ğŸ§® Word Count ì˜ˆì œ: `groupByKey()` vs `reduceByKey()`
---------------------------------------------------

##### 1\. `groupByKey()`ë¥¼ ì´ìš©í•œ Word Count

`groupByKey()`ëŠ” ë™ì¼í•œ í‚¤ë¥¼ ê°€ì§„ ëª¨ë“  ê°’ì„ ê·¸ë£¹í™”í•˜ì—¬ `(key, Iterable[values])` í˜•íƒœì˜ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹[Apache Spark](https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com)

```python
# í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ê³  ê° ì¤„ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬
lines = sc.textFile("path/to/file.txt")
words = lines.flatMap(lambda line: line.split())

# ê° ë‹¨ì–´ë¥¼ (word, 1) í˜•íƒœë¡œ ë§¤í•‘
word_pairs = words.map(lambda word: (word, 1))

# ë‹¨ì–´ë³„ë¡œ ê°’ì„ ê·¸ë£¹í™”
grouped = word_pairs.groupByKey()

# ê° ë‹¨ì–´ì˜ ì´ ì¶œí˜„ íšŸìˆ˜ë¥¼ ê³„ì‚°
word_counts = grouped.map(lambda word_group: (word_group[0], sum(word_group[1])))

# ê²°ê³¼ ì¶œë ¥
for word, count in word_counts.collect():
    print(f"{word}: {count}")
```

**ì£¼ì˜ì‚¬í•­**:

*   `groupByKey()`ëŠ” ëª¨ë“  ê°’ì„ ì…”í”Œë§í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì´ë™ì‹œí‚¤ë¯€ë¡œ, ë°ì´í„° ì–‘ì´ ë§ì„ ê²½ìš° ì„±ëŠ¥ ì €í•˜ì™€ ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹
    

##### 2\. `reduceByKey()`ë¥¼ ì´ìš©í•œ Word Count

`reduceByKey()`ëŠ” ë™ì¼í•œ í‚¤ë¥¼ ê°€ì§„ ê°’ì„ ì§€ì •ëœ í•¨ìˆ˜ë¡œ ë³‘í•©í•˜ì—¬ `(key, aggregated_value)` í˜•íƒœì˜ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹[Spark By {Examples}+2Apache Spark+2Databricks Community+2](https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com)

```python
# í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ê³  ê° ì¤„ì„ ë‹¨ì–´ë¡œ ë¶„ë¦¬
lines = sc.textFile("path/to/file.txt")
words = lines.flatMap(lambda line: line.split())

# ê° ë‹¨ì–´ë¥¼ (word, 1) í˜•íƒœë¡œ ë§¤í•‘
word_pairs = words.map(lambda word: (word, 1))

# ë‹¨ì–´ë³„ë¡œ ê°’ì„ ë³‘í•©í•˜ì—¬ ì´ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

# ê²°ê³¼ ì¶œë ¥
for word, count in word_counts.collect():
    print(f"{word}: {count}")
```

**ì¥ì **:

*   `reduceByKey()`ëŠ” ê° íŒŒí‹°ì…˜ ë‚´ì—ì„œ ë¡œì»¬ ë³‘í•©ì„ ìˆ˜í–‰í•œ í›„ ì…”í”Œë§í•˜ë¯€ë¡œ, ë„¤íŠ¸ì›Œí¬ I/Oë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.â€‹
    


##### âš–ï¸ `groupByKey()` vs `reduceByKey()` ë¹„êµ
---------------------------------------

| í•­ëª©          | `groupByKey()`                                          | `reduceByKey()`                                |
| ------------- | ------------------------------------------------------- | ---------------------------------------------- |
| ì—°ì‚° ë°©ì‹     | í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ê°’ì„ ê·¸ë£¹í™”í•˜ì—¬ Iterable ìƒì„±        | í‚¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°’ì„ ë³‘í•©í•˜ì—¬ ë‹¨ì¼ ê°’ ìƒì„±       |
| ì…”í”Œë§        | ëª¨ë“  ê°’ì„ ì…”í”Œë§í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ ì¦ê°€                 | ë¡œì»¬ ë³‘í•© í›„ ìµœì†Œí•œì˜ ì…”í”Œë§ ìˆ˜í–‰              |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ê·¸ë£¹í™”ëœ ê°’ì´ ë§ì„ ê²½ìš° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€              | ë³‘í•©ëœ ë‹¨ì¼ ê°’ë§Œ ìœ ì§€í•˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ |
| ì‚¬ìš© ì‹œê¸°     | ëª¨ë“  ê°’ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ” ê²½ìš° (ì˜ˆ: í‰ê· , ì¤‘ê°„ê°’ ê³„ì‚° ë“±) | í•©ê³„, ìµœëŒ€ê°’ ë“± ë³‘í•© ê°€ëŠ¥í•œ ì—°ì‚°ì— ì í•©        |
| ì„±ëŠ¥          | ë°ì´í„° ì–‘ì´ ë§ì„ ê²½ìš° ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥                    | ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì— íš¨ìœ¨ì                     |

* * *

##### âœ… ê²°ë¡ 
----

*   ë‹¨ì–´ ìˆ˜ë¥¼ ì„¸ëŠ” ë“±ì˜ ì§‘ê³„ ì‘ì—…ì—ëŠ” `reduceByKey()`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ê³¼ ìì› íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œ ìœ ë¦¬í•©ë‹ˆë‹¤.
    
*   ëª¨ë“  ê°’ì„ ìœ ì§€í•´ì•¼ í•˜ëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ì—ëŠ” `groupByKey()`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆì§€ë§Œ, ë°ì´í„° ì–‘ì´ ë§ì„ ê²½ìš° ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.â€‹


#### UNION

**ì •ì˜**: ë‘ ê°œì˜ RDDë¥¼ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ RDDë¡œ ë§Œë“­ë‹ˆë‹¤. ì¤‘ë³µëœ ìš”ì†Œë„ í¬í•¨ë©ë‹ˆë‹¤.â€‹

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
rdd1 = sc.parallelize(["apple", "banana"])  # ì²« ë²ˆì§¸ RDD ìƒì„±
rdd2 = sc.parallelize(["banana", "cherry"])  # ë‘ ë²ˆì§¸ RDD ìƒì„±
union_rdd = rdd1.union(rdd2)  # ë‘ RDDë¥¼ í•©ì¹¨ (ì¤‘ë³µ í—ˆìš©)
result = union_rdd.collect()
print(result)  # ì¶œë ¥: ['apple', 'banana', 'banana', 'cherry']
```

**íŠ¹ì§•**:

*   ì…ë ¥ RDDë“¤ì˜ ë°ì´í„° íƒ€ì…ì´ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤.
    
*   ì¤‘ë³µëœ ìš”ì†Œë¥¼ ì œê±°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¤‘ë³µ ì œê±°ë¥¼ ì›í•œë‹¤ë©´ `distinct()`ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.â€‹
    


#### DISTINCT

**ì •ì˜**: RDDì—ì„œ ì¤‘ë³µëœ ìš”ì†Œë¥¼ ì œê±°í•˜ì—¬ ê³ ìœ í•œ ìš”ì†Œë§Œ í¬í•¨í•˜ëŠ” ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.â€‹

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
rdd = sc.parallelize(["apple", "banana", "apple", "cherry"])  # ì¤‘ë³µëœ ìš”ì†Œ í¬í•¨ RDD ìƒì„±
distinct_rdd = rdd.distinct()  # ì¤‘ë³µ ì œê±°
result = distinct_rdd.collect()
print(result)  # ì¶œë ¥: ['apple', 'banana', 'cherry']
```

**íŠ¹ì§•**:

*   ëª¨ë“  ë°ì´í„°ë¥¼ ì…”í”Œë§í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•˜ë¯€ë¡œ, ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œëŠ” ì„±ëŠ¥ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
*   í•„ìš”í•œ ê²½ìš° íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: `rdd.distinct(numPartitions)`â€‹[Stack Overflow](https://stackoverflow.com/questions/31610971/spark-repartition-vs-coalesce?utm_source=chatgpt.com)
    

#### COALESCE

**ì •ì˜**: RDDì˜ íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ì¤„ì—¬ì„œ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì£¼ë¡œ ì¶œë ¥ íŒŒì¼ ìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì ˆì•½í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
rdd = sc.parallelize(range(1, 11), 4)  # 1ë¶€í„° 10ê¹Œì§€ ë°ì´í„°ë¥¼ 4ê°œì˜ íŒŒí‹°ì…˜ìœ¼ë¡œ ë¶„í• í•˜ì—¬ RDD ìƒì„±
coalesced_rdd = rdd.coalesce(2)  # íŒŒí‹°ì…˜ ìˆ˜ë¥¼ 2ê°œë¡œ ì¤„ì„
result = coalesced_rdd.getNumPartitions()
print(result)  # ì¶œë ¥: 2
```

**íŠ¹ì§•**:

*   ê¸°ì¡´ íŒŒí‹°ì…˜ì„ ë³‘í•©í•˜ì—¬ ìƒˆë¡œìš´ íŒŒí‹°ì…˜ì„ ìƒì„±í•˜ë¯€ë¡œ, ì…”í”Œë§ì´ ë°œìƒí•˜ì§€ ì•Šì•„ íš¨ìœ¨ì ì…ë‹ˆë‹¤.
    
*   ê¸°ë³¸ì ìœ¼ë¡œ ì…”í”Œë§ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì§€ë§Œ, `shuffle = true` ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì…”í”Œë§ì„ í†µí•´ ë” ê· ë“±í•œ íŒŒí‹°ì…˜ ë¶„ë°°ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.â€‹
    
#### REPARTITION
*   **ê¸°ëŠ¥**: íŒŒí‹°ì…˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆìœ¼ë©°, ë‚´ë¶€ì ìœ¼ë¡œ í•­ìƒ **ì „ì²´ ì…”í”Œë§**ì„ ë°œìƒì‹œí‚´.
    
*   **ìš©ë„**: ë°ì´í„° ë¶„ì‚°ì´ ë¶ˆê· í˜•í•˜ê±°ë‚˜ íŒŒí‹°ì…˜ ìˆ˜ê°€ ë„ˆë¬´ ì ì„ ë•Œ, ë³‘ë ¬ì„±ì„ ë†’ì´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©.

```python
rdd = sc.parallelize(range(1, 11), 2)  # ì´ˆê¸° 2ê°œ íŒŒí‹°ì…˜
reparted_rdd = rdd.repartition(4)     # 4ê°œë¡œ ì¬ë¶„ë°° (ì „ì²´ ì…”í”Œ ë°œìƒ)
print(reparted_rdd.getNumPartitions())  # ì¶œë ¥: 4
```

#### ğŸ“Œ `repartition()` vs `coalesce()` ë¹„êµ ìš”ì•½
----------------------------------------

| í•­ëª©               | `repartition()`                            | `coalesce()`                                |
| ------------------ | ------------------------------------------ | ------------------------------------------- |
| ëª©ì                | íŒŒí‹°ì…˜ ìˆ˜ **ëŠ˜ë¦¬ê¸° ë˜ëŠ” ì¤„ì´ê¸°** ëª¨ë‘ ê°€ëŠ¥ | íŒŒí‹°ì…˜ ìˆ˜ **ì¤„ì´ê¸° ì „ìš©**                   |
| ì…”í”Œ ë°œìƒ ì—¬ë¶€     | í•­ìƒ ì…”í”Œ ë°œìƒ (ëª¨ë“  ë°ì´í„°ë¥¼ ì¬ë¶„ë°°)      | ê¸°ë³¸ì ìœ¼ë¡œ **ì…”í”Œ ì—†ìŒ** (ì„ íƒì  ë°œìƒ ê°€ëŠ¥) |
| ë°ì´í„° ì¬ë°°ì¹˜ ë°©ì‹ | **ì „ì²´ ì…”í”Œë§**í•˜ì—¬ ê· ë“± ë¶„ì‚°              | ì¼ë¶€ íŒŒí‹°ì…˜ë§Œ ë³‘í•© (ë°ì´í„° ì ë¦¼ ê°€ëŠ¥)       |
| ì‚¬ìš© ì˜ˆì‹œ          | ì‘ì—… ë³‘ë ¬ì„± ì¦ê°€ ë˜ëŠ” ê³ ë¥´ê²Œ ë‚˜ëˆŒ ë•Œ       | ì‘ì—… ë‹¨ìˆœí™”, ì¶œë ¥ íŒŒì¼ ìˆ˜ ì¤„ì¼ ë•Œ           |
| ì„±ëŠ¥               | ëŠë¦´ ìˆ˜ ìˆìŒ (ì…”í”Œ ë¹„ìš© í¼)                | ìƒëŒ€ì ìœ¼ë¡œ ë¹ ë¦„ (ì…”í”Œ ì—†ì´ ì²˜ë¦¬ ê°€ëŠ¥)       |

##### âœ… ì‹¤ì „ íŒ
------

*   `repartition()`ì€ **ì •ë ¬ ì‘ì—… ì „** ë°ì´í„° ê³ ë¥¸ ë¶„ì‚°ì´ ì¤‘ìš”í•  ë•Œ ìœ ìš©
    
*   `coalesce()`ëŠ” **ì¶œë ¥ ìµœì í™”**(ì˜ˆ: í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥ ë“±) ì‹œ ìì£¼ ì‚¬ìš©

#### PARTITIONBY

**ì •ì˜**: Pair RDDë¥¼ íŠ¹ì • íŒŒí‹°ì…”ë„ˆì— ë”°ë¼ ì¬ë¶„ë°°í•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì£¼ë¡œ í‚¤ ê¸°ë°˜ ì—°ì‚°ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹

**ì‚¬ìš© ì˜ˆì‹œ**:

```python
pair_rdd = sc.parallelize([("apple", 1), ("banana", 2), ("cherry", 3)])  # (key, value) í˜•íƒœì˜ RDD ìƒì„±
partitioned_rdd = pair_rdd.partitionBy(2, lambda key: hash(key))  # í•´ì‹œ íŒŒí‹°ì…”ë„ˆë¡œ 2ê°œì˜ íŒŒí‹°ì…˜ ë¶„ë°°
result = partitioned_rdd.getNumPartitions()
print(result)  # ì¶œë ¥: 2
```

**íŠ¹ì§•**:

*   `HashPartitioner`ë‚˜ `RangePartitioner`ì™€ ê°™ì€ íŒŒí‹°ì…”ë„ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„ë°°í•©ë‹ˆë‹¤.
    
*   `partitionBy()`ë¥¼ ì‚¬ìš©í•˜ë©´ ì´í›„ì˜ `groupByKey()`, `reduceByKey()` ë“±ì˜ ì—°ì‚°ì—ì„œ ì…”í”Œë§ì„ ì¤„ì¼ ìˆ˜ ìˆì–´ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.
    
*   `partitionBy()`ëŠ” Pair RDDì—ë§Œ ì ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.â€‹

### 2.3 RSS Fundamentals

#### Sample

`sample()` í•¨ìˆ˜ëŠ” RDDì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ìƒˆë¡œìš´ RDDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.     
ì´ í•¨ìˆ˜ëŠ” ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ë¶„ì„í•˜ê±°ë‚˜ í…ŒìŠ¤íŠ¸í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.â€‹ 

```python
# withReplacement: ë³µì› ì¶”ì¶œ ì—¬ë¶€ (True/False)
# fraction: ì „ì²´ ë°ì´í„°ì—ì„œ ìƒ˜í”Œë§í•  ë¹„ìœ¨ (0.0 ~ 1.0)
# seed: ë‚œìˆ˜ ìƒì„± ì‹œë“œ ê°’
sampled_rdd = rdd.sample(withReplacement=False, fraction=0.1, seed=42)
```

#### Collect

`collect()` í•¨ìˆ˜ëŠ” RDDì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë“œë¼ì´ë²„ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹

```python
data = rdd.collect()
```

**ì£¼ì˜**: ëŒ€ê·œëª¨ RDDì— ëŒ€í•´ `collect()`ë¥¼ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

#### CountByKey

`countByKey()` í•¨ìˆ˜ëŠ” (K, V) í˜•íƒœì˜ RDDì—ì„œ ê° í‚¤ì— ëŒ€í•œ ê°’ì˜ ê°œìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize([("a", 1), ("b", 2), ("a", 3)])
counts = rdd.countByKey()
# ê²°ê³¼: {'a': 2, 'b': 1}
```

#### Reduce

`reduce()` í•¨ìˆ˜ëŠ” RDDì˜ ìš”ì†Œë“¤ì„ ì§€ì •ëœ ì´í•­ í•¨ìˆ˜ë¡œ ì§‘ê³„í•˜ì—¬ ë‹¨ì¼ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
rdd = sc.parallelize([1, 2, 3, 4])
total = rdd.reduce(lambda x, y: x + y)
# ê²°ê³¼: 10
```

#### Sum

`sum()` í•¨ìˆ˜ëŠ” RDDì˜ ëª¨ë“  ìˆ«ì ìš”ì†Œë¥¼ í•©ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
total = rdd.sum()
```

#### Max

`max()` í•¨ìˆ˜ëŠ” RDDì—ì„œ ìµœëŒ€ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
maximum = rdd.max()
```

#### Mean

`mean()` í•¨ìˆ˜ëŠ” RDDì˜ í‰ê· ê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
average = rdd.mean()
```

#### Stdev

`stdev()` í•¨ìˆ˜ëŠ” RDDì˜ í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
std_deviation = rdd.stdev()
```

### 2.4 RDD Action
RDDì˜ ì•¡ì…˜ ì—°ì‚°ì€ ì§€ì—° í‰ê°€ëœ ë³€í™˜ ì—°ì‚°ë“¤ì„ ì‹¤ì œë¡œ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ê±°ë‚˜ ì™¸ë¶€ ì €ì¥ì†Œì— ì €ì¥í•˜ëŠ” ì—°ì‚°ì…ë‹ˆë‹¤.â€‹
#### ACTIONS
##### 1\. `collect()`

RDDì˜ ëª¨ë“  ìš”ì†Œë¥¼ ë“œë¼ì´ë²„ í”„ë¡œê·¸ë¨ìœ¼ë¡œ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
data = rdd.collect()
```

**ì£¼ì˜**: ëŒ€ê·œëª¨ RDDì— ëŒ€í•´ `collect()`ë¥¼ ì‚¬ìš©í•˜ë©´ ë©”ëª¨ë¦¬ ë¶€ì¡±ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹

##### 2\. `count()`

RDDì˜ ìš”ì†Œ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹[Apache Spark](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.countByKey.html?utm_source=chatgpt.com)

```python
num_elements = rdd.count()
```

##### 3\. `first()`

RDDì˜ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
first_element = rdd.first()
```

##### 4\. `take(n)`

RDDì˜ ì²˜ìŒ `n`ê°œì˜ ìš”ì†Œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
first_n = rdd.take(5)
```

##### 5\. `takeSample(withReplacement, num, [seed])`

RDDì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œì„ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
sample = rdd.takeSample(False, 3, seed=42)
```

##### 6\. `reduce(func)`

RDDì˜ ìš”ì†Œë“¤ì„ ì§€ì •ëœ ì´í•­ í•¨ìˆ˜ë¡œ ì§‘ê³„í•˜ì—¬ ë‹¨ì¼ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.â€‹

```python
total = rdd.reduce(lambda x, y: x + y)
```

##### 7\. `foreach(func)`

RDDì˜ ê° ìš”ì†Œì— í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤. ì£¼ë¡œ ì™¸ë¶€ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™ì´ë‚˜ ë¡œê·¸ ì¶œë ¥ ë“±ì— ì‚¬ìš©ë©ë‹ˆë‹¤.â€‹[Apache Spark+1Learn R, Python & Data Science Online+1](https://spark.apache.org/docs/latest/rdd-programming-guide.html?utm_source=chatgpt.com)

```python
rdd.foreach(lambda x: print(x))
```

#### RDD ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°ì™€ ì €ì¥í•˜ê¸°
RDDì˜ ë°ì´í„°ë¥¼ ì™¸ë¶€ ì €ì¥ì†Œì— ì €ì¥í•  ìˆ˜ ìˆëŠ” ì•¡ì…˜ ì—°ì‚°ì…ë‹ˆë‹¤.â€‹

##### 1\. `saveAsTextFile(path)`

RDDë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ê° ìš”ì†ŒëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜ë˜ì–´ í•œ ì¤„ì”© ì €ì¥ë©ë‹ˆë‹¤.â€‹

```python
rdd.saveAsTextFile("output/path")
```

**íŠ¹ì§•**:

*   ë””ë ‰í† ë¦¬ í˜•íƒœë¡œ ì €ì¥ë˜ë©°, ê° íŒŒí‹°ì…˜ì€ ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    
*   ì••ì¶• ì½”ë±ì„ ì§€ì •í•˜ì—¬ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹
    

##### 2. `saveAsSequenceFile(path)`

(í‚¤, ê°’) í˜•íƒœì˜ RDDë¥¼ Hadoop SequenceFile í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.â€‹[Stack Overflow+2george-jen.gitbook.io+2Medium+2](https://george-jen.gitbook.io/data-science-and-apache-spark/rdd-action-functions?utm_source=chatgpt.com)

```python
pair_rdd.saveAsSequenceFile("output/sequence")
```

**ì£¼ì˜**: RDDì˜ ìš”ì†Œê°€ (í‚¤, ê°’) í˜•íƒœì—¬ì•¼ í•˜ë©°, í‚¤ì™€ ê°’ì€ Hadoopì˜ Writable ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.â€‹[george-jen.gitbook.io](https://george-jen.gitbook.io/data-science-and-apache-spark/rdd-action-functions?utm_source=chatgpt.com)

##### 3. `saveAsObjectFile(path)`

RDDì˜ ìš”ì†Œë¥¼ Java ì§ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì²´ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.â€‹

```python
rdd.saveAsObjectFile("output/object")
```

**íŠ¹ì§•**:

*   Javaì™€ Scalaì—ì„œ ì§€ì›ë˜ë©°, Pythonì—ì„œëŠ” ì‚¬ìš©ì´ ì œí•œì ì…ë‹ˆë‹¤.
    
*   ì €ì¥ëœ íŒŒì¼ì€ `SparkContext.objectFile()`ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ì½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.â€‹[Stack Overflow+2Medium+2george-jen.gitbook.io+2](https://medium.com/%40singhsameer121295/exploring-pyspark-rdds-saving-data-in-different-file-formats-6fba8c07d45c?utm_source=chatgpt.com)[george-jen.gitbook.io](https://george-jen.gitbook.io/data-science-and-apache-spark/rdd-action-functions?utm_source=chatgpt.com)
    