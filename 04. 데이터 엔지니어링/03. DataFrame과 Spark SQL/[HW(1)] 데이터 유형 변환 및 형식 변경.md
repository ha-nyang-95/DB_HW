# 데이터 유형 변환 및 형식 변경

📘 1. 실습 주제 개요 (이론, 목적, 왜 배우는지)
-------------------------------

**실습명: PySpark를 활용한 데이터 형식 변환 및 날짜 처리 실습**

현대의 데이터 분석 환경은 대용량의 데이터를 빠르게 처리하고 유연하게 조작할 수 있는 능력을 요구합니다. 특히, 데이터의 형식을 적절히 변환하고 날짜와 같은 시간 정보를 올바르게 처리하는 역량은 분석의 정확도와 효율성을 좌우합니다.

이번 실습의 주제는 PySpark 환경에서 다음과 같은 핵심 기술을 익히는 것입니다.

*   문자열 형식의 데이터를 정수형으로 변환하는 **데이터 타입 변환**
    
*   문자열로 저장된 날짜 데이터를 날짜 형식(`yyyy-MM-dd`)으로 변환하는 **날짜 포맷 처리**
    

이러한 실습을 통해, 분산 처리 환경에서도 안정적으로 데이터를 전처리할 수 있는 기초 역량을 확보할 수 있으며, 이는 향후 데이터 분석 업무 또는 대용량 로그 처리 프로젝트에서 실질적인 경쟁력으로 이어집니다.

<br>

🛠️ 2. 코드 구조 및 흐름 해설 + 실행 결과 예시 및 해설
------------------------------------

### ✅ 전체 흐름 요약

1.  SparkSession을 생성하여 PySpark 작업 환경을 구성합니다.
    
2.  리스트 형태의 샘플 데이터를 생성합니다. (이름, 날짜, 급여 정보)
    
3.  이 데이터를 DataFrame으로 변환합니다.
    
4.  문자열 형태로 저장된 급여를 정수형(IntegerType)으로 변환합니다.
    
5.  날짜 문자열을 PySpark의 날짜 타입(`yyyy-MM-dd`)으로 변환합니다.
    
6.  변환 결과를 출력하여 적용 여부를 확인합니다.
    

<br>

### 📌 실행 흐름별 주요 코드와 출력 예시

#### ✅ 1단계: 초기 데이터 생성

```python
data = [
    ("Alice", "25-12-2023", "3500"),
    ("Bob", "01-01-2024", "4200"),
    ("Charlie", "15-06-2022", "3900")
]
columns = ["name", "join_date", "salary"]
df = spark.createDataFrame(data, columns)
```

💡 출력 예시:

```
+-------+----------+------+
|   name| join_date|salary|
+-------+----------+------+
|  Alice|25-12-2023|  3500|
|    Bob|01-01-2024|  4200|
|Charlie|15-06-2022|  3900|
+-------+----------+------+
```

#### ✅ 2단계: 형 변환 수행

```python
df_casted = df.withColumn("salary", col("salary").cast(IntegerType()))
df_casted = df_casted.withColumn("join_date", to_date(col("join_date"), "dd-MM-yyyy"))
```

-> 🔍 `cast()`는 데이터 타입을 변환하며, `to_date()`는 문자열을 날짜로 바꿔줍니다. 이때 날짜 형식은 반드시 정확히 지정해야 합니다.

💡 출력 예시:

```
+-------+----------+------+
|   name| join_date|salary|
+-------+----------+------+
|  Alice|2023-12-25|  3500|
|    Bob|2024-01-01|  4200|
|Charlie|2022-06-15|  3900|
+-------+----------+------+
```

<br>

⚙️ 3. 전체 코드 + 상세 주석
-------------------

```python
# PySpark의 핵심 모듈들을 불러온다
from pyspark.sql import SparkSession                     # SparkSession: PySpark의 진입점
from pyspark.sql.functions import col, to_date           # col: 컬럼을 다룰 때 사용, to_date: 문자열을 날짜로 변환하는 함수
from pyspark.sql.types import IntegerType                # IntegerType: 정수형 데이터 타입 지정

# 1. Spark 세션 생성: 이 코드를 실행하는 동안 Spark의 기능을 사용할 수 있게 해주는 기본 환경 설정
spark = SparkSession.builder.appName("CastAndDateFormat").getOrCreate()

# 2. 예제 데이터 준비: 튜플 형태로 고객명, 주문일자, 가격을 구성
data = [
    ("Alice", "25-12-2023", "3500"),
    ("Bob", "10-01-2024", "4500"),
    ("Charlie", "05-02-2024", "5000"),
    ("David", "15-03-2024", "6000")
]

# 3. 각 튜플의 항목에 대한 컬럼 이름을 지정
columns = ["Customer", "OrderDate", "Price"]

# 4. Spark DataFrame 생성: 주어진 데이터와 컬럼 이름으로 구조화된 테이블 형식의 DataFrame 생성
df = spark.createDataFrame(data, columns)

# 5. 원본 스키마 출력: 각 컬럼의 데이터 타입 확인 (초기에는 모든 값이 문자열로 인식됨)
print("원본 스키마:")
df.printSchema()

# 6. 'Price' 컬럼을 문자열에서 정수형(IntegerType)으로 변환하여 'Price_Int'라는 새로운 컬럼으로 추가
df = df.withColumn("Price_Int", col("Price").cast(IntegerType()))

# 7. 'OrderDate' 컬럼을 문자열에서 날짜(DateType) 형식으로 변환하여 'Order_Date_Format' 컬럼으로 추가
#    'dd-MM-yyyy' 형식을 정확히 지정해야 변환이 성공함
df = df.withColumn("Order_Date_Format", to_date(col("OrderDate"), "dd-MM-yyyy"))

# 8. 변경된 스키마 출력: 변환된 컬럼들의 타입 확인
print("변환된 스키마:")
df.printSchema()

# 9. 최종 결과 출력: 각 고객의 이름, 원래 데이터, 변환된 가격 및 날짜 형식을 포함한 테이블 출력
df.show(truncate=False)
```

위 코드는 **PySpark를 이용한 전형적인 데이터 타입 처리 흐름**을 따르고 있으며, 실무에서 로그, 금융거래, 고객 행동 데이터 등 다양한 도메인에서 동일한 패턴으로 활용됩니다.

<br>

📚 4. 추가 설명 및 실무 팁 (자주 하는 실수, 심화 방향 등)
--------------------------------------

### 🔍 실수 주의사항 및 헷갈리기 쉬운 부분

#### 1️⃣ **날짜 형식 문자열 변환 시 포맷 주의**

*   `to_date(col("OrderDate"), "dd-MM-yyyy")` 에서 `"dd-MM-yyyy"`는 문자열 형식을 의미합니다.
    
    *   `"25-12-2023"` → `25`일, `12`월, `2023`년으로 해석
        
*   날짜 포맷이 실제 데이터와 다르면 `null`로 처리되므로, **항상 실제 데이터와 포맷이 일치하는지 확인**해야 합니다.
    
*   예) `"yyyy-MM-dd"`로 쓰면 변환이 실패하고, 결과는 `null`이 됩니다.
    

#### 2️⃣ **데이터 타입 변환 시 cast()와 withColumn()을 함께 사용**

*   `cast()`는 단독으로 사용할 수 없고, **`withColumn()`과 함께 사용**하여 컬럼을 새롭게 정의해줘야 합니다.
    
*   `col("Price").cast(IntegerType())` → `Price` 컬럼의 문자열 데이터를 정수로 변환
    
*   만약 잘못된 문자열이 섞여 있다면 오류가 나지 않고 `null`로 변환됩니다.
    

#### 3️⃣ **printSchema()는 확인용**

*   스키마를 출력해서 **컬럼 이름과 타입이 예상대로 변환되었는지 꼭 확인**하는 습관을 들이세요.
    
*   데이터 전처리에서 타입 오류는 후속 분석에서 연쇄적인 문제를 일으키므로, 사전 확인은 매우 중요합니다.
    

<br>

### 🚀 실무에서의 활용 및 확장 방향

#### 💼 실무 활용 사례

*   로그 데이터 처리 시 `timestamp` 문자열을 날짜 형식으로 바꾸고, 기간 필터링에 사용
    
*   금융 시스템에서는 금액 데이터가 문자열로 저장되어 있어 정수 또는 실수형으로 변환해야 정확한 계산 가능
    
*   고객 행동 분석 시 날짜별 그룹핑(예: 월별 주문수, 주간 평균 매출 등)을 위해 날짜 형식 변환은 필수
    

#### 📈 추가로 공부해보면 좋은 내용

| 주제                  | 이유                                                            |
| --------------------- | --------------------------------------------------------------- |
| `date_format()` 함수  | 날짜를 원하는 포맷으로 다시 출력할 수 있음 (예: `"yyyy/MM"` 등) |
| `unix_timestamp()`    | 타임스탬프를 정수값으로 바꿔 시간 간격 계산에 활용 가능         |
| `withColumnRenamed()` | 컬럼 이름 변경 방법도 함께 익혀두면 편리                        |
| `selectExpr()`        | SQL 문법처럼 변환하고 계산하는 고급 표현 가능                   |

<br>

### ✅ 요약 마무리

이번 실습은 PySpark 환경에서 **문자열 데이터를 실무에 적합한 형식으로 바꾸는 과정**을 이해하고, 실제 데이터를 다룰 때 꼭 필요한 기초 역량을 다지는 데 중점을 두었습니다. 간단한 예제였지만, 실제 현업에서는 수천~수백만 건의 데이터가 다뤄지고, 이때 정확한 데이터 타입과 형식 처리 능력이 실무 성과를 결정짓는 중요한 요소가 됩니다.

앞으로 더 복잡한 데이터 분석이나 ETL(Extract-Transform-Load) 작업으로 나아가기 위해서는 이러한 전처리 기초를 정확하게 익혀두는 것이 매우 중요합니다.
