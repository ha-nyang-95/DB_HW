# Flinkì™€ Sparkì˜ ì°¨ì´ì  ì´í•´ ë° ì—°ê³„

ğŸ“˜ 1. ì‹¤ìŠµ ì£¼ì œ ê°œìš”: PyFlinkë¥¼ í™œìš©í•œ ê¸ˆìœµ ë°ì´í„° ì§‘ê³„ ë¶„ì„
=========================================

ğŸ’¡ ì£¼ì œ: Flink Table APIë¡œ ìˆ˜í–‰í•˜ëŠ” ì„¹í„°ë³„ ê±°ë˜ ë°ì´í„° ì§‘ê³„
------------------------------------------

### ğŸ¯ ì‹¤ìŠµì˜ ëª©ì 

ë³¸ ì‹¤ìŠµì€ **PyFlinkë¥¼ í™œìš©í•˜ì—¬ ê¸ˆìœµ ë°ì´í„°ë¥¼ ë°°ì¹˜ ì²˜ë¦¬**í•˜ê³ , **ì„¹í„°(Sector)ë³„ ì§‘ê³„ í†µê³„ë¥¼ ê³„ì‚°**í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. Flink Table APIì™€ SQLì„ í†µí•´ CSV íŒŒì¼ë¡œë¶€í„° ë°ì´í„°ë¥¼ ì½ê³ , ì§‘ê³„ ì—°ì‚° í›„ ë‹¤ë¥¸ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì„ ì‹¤ìŠµí•œë‹¤.

### ğŸ§  ì™œ ì´ê±¸ ë°°ìš°ëŠ”ê°€?

*   **ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ SQLë¡œ ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥**ì€ ë°ì´í„° ë¶„ì„ ë° ì—”ì§€ë‹ˆì–´ë§ì˜ í•µì‹¬ì´ë‹¤.
    
*   FlinkëŠ” ê³ ì„±ëŠ¥ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ì´ì§€ë§Œ, **ë°°ì¹˜ ì²˜ë¦¬(Batch Mode)** ë˜í•œ ì§€ì›í•˜ë©° ì‹¤ë¬´ì—ì„œëŠ” ëŒ€ìš©ëŸ‰ CSV, Parquet ë“±ì˜ íŒŒì¼ ê¸°ë°˜ ë¶„ì„ì—ë„ ìì£¼ ì‚¬ìš©ëœë‹¤.
    
*   ì´ ì‹¤ìŠµì„ í†µí•´ Flink Table APIì˜ **DDL ì •ì˜, SQL ì§‘ê³„, íŒŒì¼ I/O ì—°ê²°**ì˜ ì „ì²´ íë¦„ì„ ì´í•´í•  ìˆ˜ ìˆë‹¤.
    

<br>
<br>

ğŸ› ï¸ 2. ì½”ë“œ êµ¬ì¡° ë° íë¦„ í•´ì„¤ + ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ ë° í•´ì„¤
====================================

âœ… ì „ì²´ íë¦„ ìš”ì•½
----------

ë³¸ ì½”ë“œëŠ” ë‹¤ìŒ 6ë‹¨ê³„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ë™í•œë‹¤:

1.  **Flink í™˜ê²½ ì„¤ì •** (ë°°ì¹˜ ëª¨ë“œë¡œ ì„¤ì •)
    
2.  **ìƒ˜í”Œ CSV ë°ì´í„° ìƒì„±**
    
3.  **ì†ŒìŠ¤ í…Œì´ë¸” ë“±ë¡ (CSV -> Table)**
    
4.  **ì‹±í¬ í…Œì´ë¸” ë“±ë¡ (ê²°ê³¼ Table -> CSV)**
    
5.  **SQL ì§‘ê³„ ìˆ˜í–‰**
    
6.  **ì§‘ê³„ ê²°ê³¼ ì¶œë ¥**
    

<br>

ğŸ” ì½”ë“œ êµ¬ì¡° í•´ì„¤
-----------

### â‘  Flink í™˜ê²½ êµ¬ì„± ë° ì„¤ì •

```python
env_settings = EnvironmentSettings.in_batch_mode()
table_env = TableEnvironment.create(env_settings)

config = table_env.get_config().get_configuration()
config.set_string("parallelism.default", "1")
```

*   `in_batch_mode()`ëŠ” í•œ ë²ˆì˜ ê³ ì •ëœ ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ì²˜ë¦¬í•˜ëŠ” **ë°°ì¹˜ ëª¨ë“œ**ë¥¼ ì§€ì •í•¨.
    
*   `parallelism=1`ì€ ì‹¤í–‰ ì‹œ ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ì¤€ì„ ë‚®ì¶° **ë‹¨ì¼ ì¶œë ¥ íŒŒì¼ ìƒì„±**ì„ ìœ ë„. ì‹¤ìŠµ ë° ë¡œì»¬ ë””ë²„ê¹… ì‹œ ìœ ìš©.
    

<br>

### â‘¡ CSV ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
def create_sample_csv():
    ...
```

*   `tempfile` ë””ë ‰í† ë¦¬ì— 1000ê°œì˜ ê°€ìƒ ê¸ˆìœµ ë°ì´í„°ë¥¼ ìƒì„±.
    
*   ê° ë°ì´í„°ëŠ” ë‹¤ìŒ í•„ë“œë¥¼ í¬í•¨:  
    `stock_code`, `sector`, `price`, `volume`, `transaction_date`
    
*   ëœë¤ìœ¼ë¡œ ìƒì„±ë˜ë©°, ë°ì´í„° ë¶„ì„ ì‹¤ìŠµì— ì‚¬ìš©ë  ê¸°ì´ˆ ìë£Œ ì œê³µ.
    

ì˜ˆì‹œ ë°ì´í„° (CSV íŒŒì¼):

```
005930,semiconductor,820133.59,53,2024-03-08
035420,internet,285893.55,184,2024-09-03
000660,biotech,921209.94,745,2024-04-26
...
```

<br>

### â‘¢ ì†ŒìŠ¤ í…Œì´ë¸” ì •ì˜ (CSV â†’ Table)

```sql
CREATE TABLE finance(
    stock_code STRING,
    sector STRING,
    price DOUBLE,
    volume INT,
    transaction_date STRING
) WITH (
    'connector' = 'filesystem',
    'path' = '{input_path}',
    'format' = 'csv',
    'csv.field-delimiter' = ',',
    'csv.ignore-parse-errors' = 'true'
)
```

*   **Table DDL** ë¬¸ë²•ì„ í†µí•´ CSV ë°ì´í„°ë¥¼ Flink Tableë¡œ ë§¤í•‘.
    
*   `csv.ignore-parse-errors` ì˜µì…˜ì„ í†µí•´ ê²°ì¸¡ê°’ ë“± ì˜ˆì™¸ ë ˆì½”ë“œ ë¬´ì‹œ ê°€ëŠ¥.
    

<br>

### â‘£ ì‹±í¬ í…Œì´ë¸” ì •ì˜ (Table â†’ CSV)

```sql
CREATE TABLE finance_semmary(
    sector STRING,
    total_value DOUBLE,
    avg_price DOUBLE,
    total_volume BIGINT,
    transaction_count BIGINT
) WITH (
    'connector' = 'filesystem',
    'path' = '{output_dir}',
    'format' = 'csv',
    'csv.field-delimiter' = ','
)
```

*   `finance_summary` í…Œì´ë¸”ì€ SQL ê²°ê³¼ê°€ ì €ì¥ë  ì¶œë ¥ìš© í…Œì´ë¸”.
    
*   DDLì—ì„œ ì§ì ‘ **íŒŒì¼ ê²½ë¡œ**ì™€ **í¬ë§·(csv)** ì„ ì§€ì •í•¨.
    

<br>

### â‘¤ SQL ì§‘ê³„ ìˆ˜í–‰ ë° ê²°ê³¼ ì¶œë ¥

```sql
SELECT
    sector,
    SUM(price * volume) as total_value,
    AVG(price) as avg_price,
    SUM(volume) as total_volume,
    COUNT(*) as transaction_count
FROM
    finance
GROUP BY
    sector
```

*   ê° ì„¹í„°ë³„ë¡œ ê±°ë˜ ìš”ì•½ í†µê³„ë¥¼ ê³„ì‚°:
    
    *   `ì´ ê±°ë˜ì•¡`: ê°€ê²© Ã— ìˆ˜ëŸ‰
        
    *   `í‰ê·  ê°€ê²©`: í‰ê·  ì£¼ê°€
        
    *   `ì´ ê±°ë˜ëŸ‰`: ê±°ë˜ ìˆ˜ëŸ‰ ì´í•©
        
    *   `ê±°ë˜ ê±´ìˆ˜`: í•´ë‹¹ ì„¹í„°ì—ì„œ ë°œìƒí•œ ê±°ë˜ íšŸìˆ˜
        

<br>

ğŸ“Œ ì¶œë ¥ ì˜ˆì‹œ (ì½˜ì†”)
-------------

```plaintext
=== ì„¹í„°ë³„ ê¸ˆìœµ ë°ì´í„° ìš”ì•½ ===
ì„¹í„°    ì´ ê±°ë˜ì•¡     í‰ê·  ê°€ê²©   ì´ ê±°ë˜ëŸ‰   ê±°ë˜ ê±´ìˆ˜
--------------------------------------------------------------------------------
biotech    211,384,993.26    536,821.33   395,482     326
internet   189,222,123.11    512,006.45   367,111     322
semiconductor  200,144,872.70  504,981.14   380,913     352
```

*   CSV ì¶œë ¥ íŒŒì¼ë„ ë™ì¼í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ë©°, `output_dir` ê²½ë¡œì— ì €ì¥ë¨.
    

<br>
<br>

âš™ï¸ 3. ì „ì²´ ì½”ë“œ + ìƒì„¸ ì£¼ì„
===================

```python
# Flink Table API í™˜ê²½ ë° íŒŒì´ì¬ ê¸°ë³¸ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from pyflink.table import EnvironmentSettings, TableEnvironment
import tempfile  # ì„ì‹œ íŒŒì¼/í´ë” ìƒì„±ì„ ìœ„í•œ ëª¨ë“ˆ
import os
import pandas as pd  # (ë°ì´í„° ê²€í† ìš© - ì‹¤ì œ ì‹¤í–‰ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
import numpy as np
import logging

# ë¡œê·¸ ì„¤ì • (ì •ë³´ ë‹¨ìœ„ ë©”ì‹œì§€ë¥¼ ì¶œë ¥)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    logger.info("Flink ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")

    # 1. Flink í™˜ê²½ ì„¤ì •: ë°°ì¹˜ ëª¨ë“œë¡œ ë™ì‘í•˜ë„ë¡ ì„¤ì •
    env_settings = EnvironmentSettings.in_batch_mode()
    table_env = TableEnvironment.create(env_settings)

    # ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜ ì„¤ì •: 1ë¡œ ì§€ì •í•˜ì—¬ ì¶œë ¥ íŒŒì¼ì´ í•˜ë‚˜ë¡œ ìƒì„±ë˜ë„ë¡ í•¨
    config = table_env.get_config().get_configuration()
    config.set_string("parallelism.default", "1")

    # 2. ìƒ˜í”Œ CSV ë°ì´í„° ìƒì„±
    input_path = create_sample_csv()

    # 3. ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì • ë° ì´ˆê¸°í™”
    output_dir = tempfile.gettempdir() + '/flink_finance_output'
    if os.path.exists(output_dir):
        import shutil
        shutil.rmtree(output_dir)  # ê¸°ì¡´ ê²°ê³¼ ì œê±°

    # 4. ì†ŒìŠ¤ í…Œì´ë¸” ì •ì˜ (CSV -> Table ë§¤í•‘)
    source_ddl = f"""
    CREATE TABLE finance(
        stock_code STRING,
        sector STRING,
        price DOUBLE,
        volume INT,
        transaction_date STRING
    ) WITH(
        'connector' = 'filesystem',
        'path' = '{input_path}',
        'format' = 'csv',
        'csv.field-delimiter' = ',',
        'csv.ignore-parse-errors' = 'true'
    )
    """
    table_env.execute_sql(source_ddl)

    # 5. ê²°ê³¼ ì €ì¥ìš© ì‹±í¬ í…Œì´ë¸” ì •ì˜ (Table -> CSV)
    sink_ddl = f"""
    CREATE TABLE finance_semmary(
        sector STRING,
        total_value DOUBLE,
        avg_price DOUBLE,
        total_volume BIGINT,
        transaction_count BIGINT
    ) WITH (
        'connector' = 'filesystem',
        'path' = '{output_dir}',
        'format' = 'csv',
        'csv.field-delimiter' = ','
    )
    """
    table_env.execute_sql(sink_ddl)

    # 6. SQL ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰ (ì„¹í„°ë³„ í†µê³„ ì§‘ê³„)
    result = table_env.execute_sql("""
    INSERT INTO finance_semmary
    SELECT
        sector,
        SUM(price * volume) as total_value,
        AVG(price) as avg_price,
        SUM(volume) as total_volume,
        COUNT(*) as transaction_count
    FROM
        finance
    GROUP BY
        sector
    """)

    # 7. ê²°ê³¼ ì¶œë ¥ (ì½˜ì†”)
    print("\n=== ì„¹í„°ë³„ ê¸ˆìœµ ë°ì´í„° ìš”ì•½ ===")
    print("ì„¹í„°\tì´ ê±°ë˜ì•¡\tí‰ê·  ê°€ê²©\tì´ ê±°ë˜ëŸ‰\tê±°ë˜ ê±´ìˆ˜")
    print("-" * 80)
    with result.collect() as results:
        for row in results:
            print(f"{row[0]}\t{row[1]:,.2f}\t{row[2]:,.2f}\t{row[3]:,}\t{row[4]:,}")

# ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì„ì‹œ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def create_sample_csv():
    temp_file = tempfile.gettempdir() + '/finance_data.csv'
    np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´

    # ìƒ˜í”Œ ì£¼ì‹ ì½”ë“œì™€ ì„¹í„° ëª©ë¡
    stock_codes = ['005930', '000660', '035420', '068270']
    sectors = ['semiconductor', 'biotech', 'internet']

    # ë°ì´í„° ìƒì„±
    data = []
    for _ in range(1000):
        stock_code = np.random.choice(stock_codes)
        sector = np.random.choice(sectors)
        price = round(np.random.uniform(50000, 1000000), 2)
        volume = np.random.randint(10, 1000)
        date = f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}"
        data.append(f"{stock_code},{sector},{price},{volume},{date}")

    # CSV íŒŒì¼ë¡œ ì €ì¥
    with open(temp_file, 'w') as f:
        f.write('\n'.join(data))

    return temp_file

# main í•¨ìˆ˜ ì‹¤í–‰
if __name__ == '__main__':
    main()
```

<br>
<br>

ğŸ“š 4. ì¶”ê°€ ì„¤ëª… ë° ì‹¤ë¬´ íŒ
==================

â— ì‹¤ìŠµ ì‹œ ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ë²•
----------------------

| ë¬¸ì œ | ì›ì¸ | í•´ê²° ë°©ì•ˆ |
| --- | --- | --- |
| **CSV íŒŒì¼ ì¸ì‹ ì˜¤ë¥˜** | ìŠ¤í‚¤ë§ˆì™€ ì‹¤ì œ ë°ì´í„° íƒ€ì… ë¶ˆì¼ì¹˜ | `csv.ignore-parse-errors = true` ì‚¬ìš©, ë˜ëŠ” `schema()`ì—ì„œ íƒ€ì… ëª…ì‹œ |
| **ì¶œë ¥ ê²°ê³¼ê°€ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë‚˜ë‰¨** | ê¸°ë³¸ ë³‘ë ¬ì„±ì´ 1 ì´ìƒ | `parallelism.default = 1`ë¡œ ì„¤ì • |
| **`collect()` í˜¸ì¶œ ì‹œ ê²°ê³¼ ì—†ìŒ** | `SELECT`ë§Œ í–ˆê³  `INSERT INTO` ëˆ„ë½ | ë°˜ë“œì‹œ `INSERT INTO` sink\_tableì„ ìˆ˜í–‰í•´ì•¼ íŒŒì¼ì— ê²°ê³¼ê°€ ì €ì¥ë¨ |
| **ì„ì‹œ íŒŒì¼ ëˆ„ë½** | `create_sample_csv()` í˜¸ì¶œ ì „ ê²½ë¡œ ë¬¸ì œ | íŒŒì¼ ê²½ë¡œ í™•ì¸ ë˜ëŠ” `tempfile` ì‚¬ìš© ê¶Œì¥ |

<br>

ğŸ’¡ ì‹¤ë¬´ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ê°€?
------------------

### âœ… ëŒ€í‘œ í™œìš© ì‚¬ë¡€

*   **ê±°ë˜ì†Œ ë°ì´í„° ë¶„ì„**: ì£¼ì‹/ì•”í˜¸í™”íì˜ ê°€ê²©, ê±°ë˜ëŸ‰, ì„¹í„°ë³„ íë¦„ ë¶„ì„
    
*   **ì‹¤ì‹œê°„ ë¦¬í¬íŠ¸ ìƒì„±**: ë§¤ì¼ ë°¤ ë˜ëŠ” ì‹œê°„ ë‹¨ìœ„ë¡œ ë°ì´í„° ì§‘ê³„ í›„ PDF/ë³´ê³ ì„œë¡œ ë³€í™˜
    
*   **ëŒ€ìš©ëŸ‰ ë¡œê·¸ ì§‘ê³„**: ì„œë²„ ë¡œê·¸ë¥¼ ë‚ ì§œÂ·IPÂ·ìš”ì²­ ìœ í˜•ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì´ìƒ íƒì§€
    
*   **Flink + Kafka ì—°ê³„**: Kafkaì—ì„œ ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„° ìˆ˜ì‹  â†’ Flinkë¡œ ì²˜ë¦¬ â†’ ì‹¤ì‹œê°„ DB ì €ì¥
    

<br>

ğŸ§­ í•™ìŠµ í™•ì¥ í¬ì¸íŠ¸
------------

| í™•ì¥ ì£¼ì œ | ì„¤ëª… |
| --- | --- |
| **Flink Streaming ëª¨ë“œë¡œ ì „í™˜** | `EnvironmentSettings.in_streaming_mode()`ë¡œ ë³€ê²½í•˜ì—¬ ì‹¤ì‹œê°„ íë¦„ ì²˜ë¦¬ |
| **Kafka ì—°ë™** | `connector = 'kafka'` ì„¤ì •ì„ í†µí•´ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ê°€ëŠ¥ |
| **Time-based Grouping** | `TUMBLE(transaction_time, INTERVAL '1' HOUR)` ë“±ìœ¼ë¡œ ì‹œê°„ ë‹¨ìœ„ ì§‘ê³„ |
| **Parquet, JSON í¬ë§·** | ê²°ê³¼ í¬ë§·ì„ CSV ì™¸ì—ë„ Parquet, JSON ë“±ìœ¼ë¡œ ë‹¤ì–‘í™”í•˜ì—¬ Sparkì™€ ì—°ê³„ ê°€ëŠ¥ |
| **Flink SQL Gateway** | ì½”ë“œ ì—†ì´ SQLë§Œìœ¼ë¡œ Flink íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìš´ì˜ í™˜ê²½ì— ì í•©) |

<br>

ğŸ” ì‹¬í™” í•™ìŠµ íŒ
----------

*   **Table API vs DataStream API**: Table APIëŠ” SQL ê¸°ë°˜ ì²˜ë¦¬ì— ê°•í•˜ê³ , DataStream APIëŠ” ìœ ì—°í•œ ì‚¬ìš©ì ì •ì˜ ì—°ì‚°ì— ì í•©í•˜ë‹¤.
    
*   **Flink SQLê³¼ Spark SQL ë¹„êµ í•™ìŠµ**: ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ì„±ëŠ¥/í™•ì¥ì„± ì°¨ì´ë¥¼ ì‹¤í—˜í•´ ë³´ë©´ í•™ìŠµ íš¨ê³¼ í¼.
    
*   **Kafka-Flink ì—°ë™ í”„ë¡œì íŠ¸**: ì‹¤ì œ RSS ë‰´ìŠ¤ë‚˜ ì£¼ê°€ ë°ì´í„°ë¥¼ Kafkaë¡œ ìˆ˜ì§‘í•˜ì—¬ Flinkë¡œ ë¶„ì„í•˜ëŠ” í”„ë¡œì íŠ¸ êµ¬ì„± ì¶”ì²œ.
    

<br>

âœ… ë§ˆë¬´ë¦¬ ìš”ì•½
--------

ì´ ì‹¤ìŠµì€ Flinkì˜ ë°°ì¹˜ í™˜ê²½ì—ì„œ Table APIì™€ SQLì„ í†µí•´ íŒŒì¼ ê¸°ë°˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì „ ê³¼ì •ì„ ë‹´ê³  ìˆë‹¤.  
ì‹¤ì œ ë°ì´í„° í™˜ê²½ì—ì„œëŠ” Kafka, Hive, JDBC ë“± ë‹¤ì–‘í•œ ì†ŒìŠ¤ì™€ ì‹±í¬ê°€ í•¨ê»˜ ì“°ì´ë©°, ë³¸ ì˜ˆì œëŠ” ê·¸ êµ¬ì¡°ë¥¼ ê°„ë‹¨í•œ í˜•íƒœë¡œ ì¶•ì†Œí•´ ì´í•´ë¥¼ ë•ëŠ”ë‹¤.

> ë°ì´í„°ë¥¼ "ì–´ë–»ê²Œ ì €ì¥í•˜ê³ ", "ì–´ë–»ê²Œ ì§‘ê³„í•˜ê³ ", "ì–´ë””ë¡œ ì „ë‹¬í• ì§€"ì— ëŒ€í•œ end-to-end ê°ê°ì„ í‚¤ìš°ëŠ” ê²ƒì´ Flink í•™ìŠµì˜ í•µì‹¬ì´ë‹¤.
