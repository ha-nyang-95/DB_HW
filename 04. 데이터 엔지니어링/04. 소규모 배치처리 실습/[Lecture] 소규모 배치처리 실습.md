# Kafka, Flink, Spark í†µí•© ì‹¤ìŠµ ì •ë¦¬ë³¸

```text
[Abstract]

ë³¸ ì •ë¦¬ë³¸ì€ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì™€ ë°°ì¹˜ ì²˜ë¦¬ì˜ ì „ë°˜ì ì¸ íë¦„ì„ ì§ì ‘ êµ¬í˜„í•˜ê³  í•™ìŠµí•˜ê¸° ìœ„í•œ ì‹¤ìŠµ ê¸°ë°˜ ë¬¸ì„œì´ë‹¤. Kafkaë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ Flinkì™€ Sparkë¥¼ ì—°ê³„í•˜ì—¬ ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ìƒì„±, ìŠ¤íŠ¸ë¦¬ë° ì§‘ê³„, ê·¸ë¦¬ê³  ì •ì  íŒŒì¼ ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ í¬ê´„í•˜ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ì˜€ë‹¤.

ë¨¼ì €, Python ê¸°ë°˜ Kafka Producerë¥¼ í†µí•´ ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„±í•˜ì—¬ Kafka í† í”½ìœ¼ë¡œ ì „ì†¡í•˜ê³ , Flink Table APIë¥¼ í™œìš©í•˜ì—¬ í•´ë‹¹ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ ë° í–‰ë™ ìœ í˜•ë³„ë¡œ ì‹¤ì‹œê°„ ì§‘ê³„í•œ í›„ ë‹¤ì‹œ Kafkaë¡œ ì¶œë ¥í•˜ëŠ” ìŠ¤íŠ¸ë¦¬ë° êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ì˜€ë‹¤. ì´ì–´ì„œ, Spark Structured Streamingì„ ì´ìš©í•´ Kafkaë¡œë¶€í„° íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ê³ , 1ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ì§‘ê³„ë¥¼ í†µí•´ ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë©”ì‹œì§€ íŒ¨í„´ì„ ì‹œê°í™”í•˜ì˜€ë‹¤.

ë˜í•œ, PyFlinkë¥¼ í™œìš©í•œ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤ìŠµì„ í†µí•´ CSV í˜•ì‹ì˜ ì •ì  íŒë§¤ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ , SQL ê¸°ë°˜ì˜ ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„ ì‘ì—…ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ Flinkì˜ ë°°ì¹˜ ì²˜ë¦¬ ì—­ëŸ‰ì„ ê²€ì¦í•˜ì˜€ë‹¤. ê²°ê³¼ ë°ì´í„°ëŠ” Pandasë¥¼ í†µí•´ í›„ì²˜ë¦¬ ë° ì‹œê°í™”ë˜ì—ˆë‹¤.

ì´ë²ˆ ì •ë¦¬ë³¸ì„ í†µí•´ Kafkaì˜ ìœ ì—°í•œ ë©”ì‹œì§€ ì „ë‹¬ êµ¬ì¡°, Flinkì˜ ì •ë°€í•œ ì‹œê°„ ì œì–´ ë° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ëŠ¥ë ¥, Sparkì˜ ì§ê´€ì ì¸ ë§ˆì´í¬ë¡œ ë°°ì¹˜ ë°©ì‹, PyFlinkì˜ SQL ê¸°ë°˜ ë°°ì¹˜ ë¶„ì„ ê¸°ëŠ¥ ë“± ê°ê°ì˜ ê¸°ìˆ ì´ ê°€ì§€ëŠ” ê³ ìœ í•œ íŠ¹ì„±ê³¼ ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„±ì„ í†µí•©ì ìœ¼ë¡œ ì¡°ë§í•  ìˆ˜ ìˆì—ˆë‹¤. ì‹¤ì‹œê°„ ë°ì´í„° í™˜ê²½ì—ì„œì˜ í†µí•©ì  ì‚¬ê³ ì™€ ê¸°ìˆ  ì„ íƒì˜ ê·¼ê±°ë¥¼ ë§ˆë ¨í•˜ê³ ì í•˜ëŠ” í•™ìŠµìì—ê²Œ ì‹¤ì§ˆì ì´ê³  êµ¬ì¡°í™”ëœ ì´í•´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤.
```

<br>

1\. ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê°œìš” ë° ëª©í‘œ
=========================


### 1.1 ë°ì´í„°ëŠ” ì™œ â€˜ì‹¤ì‹œê°„â€™ì´ì–´ì•¼ í•˜ëŠ”ê°€?

í˜„ëŒ€ì˜ ë””ì§€í„¸ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì—ì„œëŠ” ë‹¨ìˆœí•œ ë°ì´í„° ìˆ˜ì§‘ê³¼ ì €ì¥ì„ ë„˜ì–´,  
**ë°ì´í„°ì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ì™€ ë¶„ì„**ì´ í•µì‹¬ ê²½ìŸë ¥ìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆë‹¤.

*   **ì‚¬ìš©ì ê²½í—˜(UX) ê°œì„ **: í˜ì´ì§€ í´ë¦­, ìƒí’ˆ íƒìƒ‰, ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸° ë“± ì‚¬ìš©ìì˜ í–‰ë™ì€ ë¹ ë¥´ê²Œ ë³€í™”í•˜ë©°, ê·¸ì— ì¦‰ì‹œ ë°˜ì‘í•˜ëŠ” ê°œì¸í™” ì „ëµì´ í•„ìš”í•˜ë‹¤.
    
*   **ì¦‰ê°ì ì¸ ì¸ì‚¬ì´íŠ¸ ì œê³µ**: ì‹¤ì‹œê°„ ë¶„ì„ì„ í†µí•´ ë§ˆì¼€íŒ…, ì¶”ì²œ ì‹œìŠ¤í…œ, íŠ¸ë˜í”½ ì¡°ì ˆ ë“±ì˜ ë°˜ì‘ ì†ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤.
    
*   **ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ**: ì‹¤ì‹œê°„ ì¥ì•  íƒì§€, ì¬ê³  ê´€ë¦¬, ë¬¼ë¥˜ ì¶”ì  ë“±ë„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¡œ ê°€ëŠ¥í•´ì§„ë‹¤.
    

ì´ëŸ¬í•œ ì‹¤ì‹œê°„ íë¦„ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´, ë³¸ ì‹¤ìŠµì—ì„œëŠ” ë‹¤ìŒì˜ êµ¬ì„±ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ì˜€ë‹¤:

<br>

### 1.2 ì‹¤ìŠµ ëª©í‘œ ë° êµ¬ì¡°

> ë°ì´í„°ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìƒì„± â†’ Kafkaë¡œ ì „ì†¡ â†’ Flink ë˜ëŠ” Sparkì—ì„œ ì²˜ë¦¬ â†’ ë‹¤ì‹œ Kafkaë¡œ ì „ì†¡

ì•„ë˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ íë¦„ì´ë‹¤:

```mermaid
graph TD
    A[ë°ì´í„° ìƒì„±ê¸° (Python)] --> B[Kafka (user_behaviors)]
    B --> C[Flink ìŠ¤íŠ¸ë¦¬ë° ì§‘ê³„]
    C --> D[Kafka (behavior_stats)]
    B --> E[Spark ì§‘ê³„ í…ŒìŠ¤íŠ¸ìš© ë©”ì‹œì§€ ì²˜ë¦¬]
```

#### ì£¼ìš” ì‹¤ìŠµ ë„êµ¬ ë° ì—­í• :

| êµ¬ì„± ìš”ì†Œ                 | ì„¤ëª…                                                       |
| ------------------------- | ---------------------------------------------------------- |
| **Kafka**                 | ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì „ë‹¬ ì‹œìŠ¤í…œ (ë©”ì‹œì§€ í)                      |
| **Flink**                 | ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì—”ì§„ (ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜ ëª¨ë‘ ì§€ì›)         |
| **Spark**                 | ë§ˆì´í¬ë¡œ ë°°ì¹˜ ê¸°ë°˜ ì‹¤ì‹œê°„ ë¶„ì„ ë„êµ¬ (Structured Streaming) |
| **Python Kafka Producer** | ì‚¬ìš©ì ì´ë²¤íŠ¸ë¥¼ Kafkaë¡œ ì „ì†¡í•˜ëŠ” ì‹œë®¬ë ˆì´ì…˜ ì½”ë“œ           |

<br>

### 1.3 ì‹¤ìŠµ ê°œìš” ìš”ì•½

| ì‹¤ìŠµ êµ¬ì„±         | ëª©ì                                                 |
| ----------------- | --------------------------------------------------- |
| **Kafka + Flink** | ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ë° ì‹¤ì‹œê°„ ì§‘ê³„            |
| **Kafka + Spark** | ì‹œê°„ ê¸°ë°˜ ë©”ì‹œì§€ ì§‘ê³„ë¥¼ í†µí•´ Spark ìŠ¤íŠ¸ë¦¬ë° ì´í•´    |
| **Flink Batch**   | ì •ì  CSV ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ì—¬ Table API ë°°ì¹˜ í™œìš© ê²½í—˜ |

<br>
<br>

2\. ì‹¤ì‹œê°„ ì‚¬ìš©ì ì´ë²¤íŠ¸ ìƒì„±: Kafka í”„ë¡œë“€ì„œ êµ¬ì„±
=================================


2.1 ì‹¤ìŠµ ëª©ì 
---------

ë³¸ ì‹¤ìŠµì—ì„œëŠ” Kafkaë¥¼ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ **ì…êµ¬(ingestion point)** ë¡œ í™œìš©í•œë‹¤.  
ì´ë¥¼ ìœ„í•´ Python ì–¸ì–´ë¡œ ì‘ì„±ëœ **Kafka Producer** ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬,  
ì‚¬ìš©ìì˜ í´ë¦­, ì¡°íšŒ, êµ¬ë§¤ ë“±ì˜ í–‰ë™ì„ ëª¨ì˜í•œ ì´ë²¤íŠ¸ë¥¼ Kafkaë¡œ ì‹¤ì‹œê°„ ì „ì†¡í•œë‹¤.

<br>

2.2 ì‹¤ìŠµ ëŒ€ìƒ í† í”½
------------

KafkaëŠ” ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ì—¬ ì €ì¥í•˜ê¸° ìœ„í•´ **í† í”½(Topic)** ì´ë¼ëŠ” ë…¼ë¦¬ì  ë‹¨ìœ„ë¥¼ ì‚¬ìš©í•œë‹¤.  
ì´ ì‹¤ìŠµì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ í† í”½ì´ ì‚¬ìš©ëœë‹¤:

*   `user_behaviors` : ì‹¤ì‹œê°„ìœ¼ë¡œ ì‚¬ìš©ì í–‰ë™ ì´ë²¤íŠ¸ë¥¼ ìˆ˜ì‹ í•˜ëŠ” Kafka í† í”½
    
*   ì´í›„ Flinkì—ì„œ ì´ ë°ì´í„°ë¥¼ êµ¬ë…í•˜ê³  ë¶„ì„
    

<br>

2.3 í•µì‹¬ ì½”ë“œ ì„¤ëª…: `flink_producer.py`


### ğŸ“„ ì „ì²´ êµ¬ì¡°

```python
from kafka import KafkaProducer
import json
import time
import random
from datetime import datetime

# Kafka í”„ë¡œë“€ì„œ ì„¤ì •
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# ìƒ˜í”Œ ë°ì´í„° ì†ì„±
user_ids = [f"user_{i}" for i in range(1, 100)]
item_ids = [f"item_{i}" for i in range(1, 200)]
categories = ["electronics", "books", "clothing", "home", "sports"]
behaviors = ["click", "view", "add_to_cart", "purchase"]

# ë°ì´í„° ìƒì„± ë° ì „ì†¡
for _ in range(1000):  # 1000ê°œì˜ ì´ë²¤íŠ¸ ìƒì„±
    event = {
        "user_id": random.choice(user_ids),
        "item_id": random.choice(item_ids),
        "category": random.choice(categories),
        "behavior": random.choice(behaviors),
        "ts": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")[:-3]
    }

    producer.send('user_behaviors', event)
    print(f"Sent: {event}")
    time.sleep(0.1)  # 0.1ì´ˆ ê°„ê²©ìœ¼ë¡œ ì´ë²¤íŠ¸ ì „ì†¡

producer.flush()
print("Data generation complete!")
```

<br>

2.4 ì½”ë“œ êµ¬ì„± ìƒì„¸ í•´ì„¤
---------------

### ğŸ”§ KafkaProducer ì„¤ì •

```python
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)
```

*   `bootstrap_servers`: Kafka ë¸Œë¡œì»¤ ì£¼ì†Œë¥¼ ì§€ì •í•œë‹¤. ì—¬ê¸°ì„œëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ ë™ì‘í•˜ë¯€ë¡œ `localhost:9092`ë¥¼ ì‚¬ìš©.
    
*   `value_serializer`: Python ë”•ì…”ë„ˆë¦¬(`dict`) í˜•íƒœì˜ ë°ì´í„°ë¥¼ Kafkaì— ì „ì†¡í•˜ê¸° ìœ„í•´,  
    ì´ë¥¼ JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”í•˜ê³  UTF-8ë¡œ ì¸ì½”ë”©í•¨.
    

### ğŸ² ì´ë²¤íŠ¸ ì†ì„± ì •ì˜

```python
user_ids = [f"user_{i}" for i in range(1, 100)]
item_ids = [f"item_{i}" for i in range(1, 200)]
categories = ["electronics", "books", "clothing", "home", "sports"]
behaviors = ["click", "view", "add_to_cart", "purchase"]
```

*   ì´ë²¤íŠ¸ì— í¬í•¨ë  ì‚¬ìš©ì ID, ìƒí’ˆ ID, ì¹´í…Œê³ ë¦¬, í–‰ë™ íƒ€ì…ì„ ë¯¸ë¦¬ ì •ì˜í•˜ì—¬ ëœë¤ ì¶”ì¶œ.
    

### ğŸ§¾ JSON ë©”ì‹œì§€ ìƒì„± ë° Kafka ì „ì†¡

```python
event = {
    "user_id": ...,
    "item_id": ...,
    "category": ...,
    "behavior": ...,
    "ts": ...
}
```

*   ê° ë©”ì‹œì§€ëŠ” ì‚¬ìš©ì í–‰ë™ì„ ë‚˜íƒ€ë‚´ë©°, `ts` í•„ë“œëŠ” millisecond ë‹¨ìœ„ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í¬í•¨í•œë‹¤.
    
*   0.1ì´ˆë§ˆë‹¤ ì´ë²¤íŠ¸ë¥¼ ìƒì„±í•´ Kafkaì˜ `user_behaviors` í† í”½ìœ¼ë¡œ ì „ì†¡í•˜ë©°,  
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í™˜ê²½ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‹œë®¬ë ˆì´ì…˜í•œë‹¤.
    

<br>

2.5 ì‹¤í–‰ ë°©ë²•
---------

í„°ë¯¸ë„ì—ì„œ í•´ë‹¹ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¡œê·¸ê°€ ì¶œë ¥ë˜ë©° Kafkaë¡œ ë©”ì‹œì§€ê°€ ì „ì†¡ëœë‹¤:

```bash
$ python flink_producer.py
Sent: {'user_id': 'user_3', 'item_id': 'item_122', 'category': 'books', 'behavior': 'view', 'ts': '2025-04-21 14:10:45.123'}
Sent: ...
...
Data generation complete!
```

<br>

2.6 ì´ ì‹¤ìŠµì˜ ì˜ë¯¸
------------

ì´ ProducerëŠ” ë‹¨ìˆœí•œ í…ŒìŠ¤íŠ¸ ì½”ë“œê°€ ì•„ë‹ˆë‹¤.  
Flinkë‚˜ Sparkì˜ ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ **í˜„ì‹¤ì ì¸ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ì— ê¸°ë°˜í•œ ì´ë²¤íŠ¸ íë¦„ìœ¼ë¡œ ê²€ì¦í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ”** í•µì‹¬ ì¸í”„ë¼ì´ë‹¤.  
ì´ë¥¼ í†µí•´ ì•„ë˜ì™€ ê°™ì€ ê²½í—˜ì„ í•˜ê²Œ ëœë‹¤:

*   Kafka ê¸°ë°˜ ë©”ì‹œì§€ íì˜ ë™ì‘ ë°©ì‹ ì²´í—˜
    
*   JSON ê¸°ë°˜ ì´ë²¤íŠ¸ êµ¬ì¡° ì„¤ê³„ ê²½í—˜
    
*   ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì „, **â€˜ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë°œìƒí•˜ëŠ”ê°€â€™** ì— ëŒ€í•œ ê°ê° í™•ë³´
    

<br>
<br>

3\. Flink ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë° ì‹¤í–‰
============================


3.1 ì‹¤ìŠµ ëª©ì 
---------

FlinkëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—”ì§„ì´ë‹¤.  
ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” Kafkaë¡œë¶€í„° ë“¤ì–´ì˜¨ ì‚¬ìš©ì ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ Flinkê°€ ë°›ì•„ë“¤ì—¬,  
ì¹´í…Œê³ ë¦¬(category)ì™€ í–‰ë™ ìœ í˜•(behavior)ë³„ë¡œ ì§‘ê³„í•˜ê³ ,  
ê·¸ ê²°ê³¼ë¥¼ ë‹¤ì‹œ Kafkaì— ë³´ë‚´ëŠ” **end-to-end ì‹¤ì‹œê°„ ë°ì´í„° íë¦„**ì„ êµ¬í˜„í•œë‹¤.

<br>

3.2 ì‹¤ìŠµ íë¦„ ìš”ì•½
------------

```mermaid
graph TD
    A[Kafka Topic: user_behaviors] --> B[Flink Table API Job]
    B --> C[Kafka Topic: behavior_stats]
```

*   `user_behaviors`: ì‚¬ìš©ì ì´ë²¤íŠ¸ê°€ ë‹´ê¸´ Kafka í† í”½ (ì…ë ¥)
    
*   `behavior_stats`: Flinkê°€ ì§‘ê³„ ê²°ê³¼ë¥¼ ë³´ë‚´ëŠ” Kafka í† í”½ (ì¶œë ¥)
    

<br>

3.3 í•µì‹¬ ì½”ë“œ êµ¬ì¡°: `kafka_flink_example.py`
--------------------------------------

### ğŸ“„ ì „ì²´ êµ¬ì¡°

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

import os
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("Flink ì‘ì—… ì‹œì‘...")
    
    # ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰ í™˜ê²½ ì„¤ì •
    env = StreamExecutionEnvironment.get_execution_environment()
    # Table APIëŠ” Batchì™€ Streaming ëª¨ë“œë¥¼ ëª…í™•íˆ êµ¬ë¶„
    # DataStream APIëŠ” ê¸°ë³¸ì´ ìŠ¤íŠ¸ë¦¬ë° â†’ ë³„ë„ ì„¤ì • í•„ìš” ì—†ìŒ
    # Table APIì—ì„œ Kafka ì‚¬ìš© ì‹œ	in_streaming_mode() + StreamTableEnvironment í•„ìˆ˜
    env_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
    table_env = StreamTableEnvironment.create(env, environment_settings=env_settings)
    
    # ë¡œê¹… ë ˆë²¨ ì„¤ì •
    table_env.get_config().get_configuration().set_string("pipeline.global-job-parameters.logger.level", "INFO")
    
    # JAR íŒŒì¼ ì¶”ê°€ (ì „ì²´ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)
    kafka_jar = os.path.join(os.path.abspath('.'), 'flink-sql-connector-kafka-3.3.0-1.20.jar')
    logger.info(f"ì‚¬ìš©í•˜ëŠ” JAR íŒŒì¼ ê²½ë¡œ: {kafka_jar}")
    if not os.path.exists(kafka_jar):
        logger.error(f"JAR íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {kafka_jar}")
        return
    
    table_env.get_config().get_configuration().set_string("pipeline.jars", f"file://{kafka_jar}")
    
    # ì†ŒìŠ¤ í…Œì´ë¸” ì •ì˜
    try:
        logger.info("Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„± ì‹œë„...")
        table_env.execute_sql("""
        CREATE TABLE kafka_source (
            user_id STRING,
            item_id STRING,
            category STRING,
            behavior STRING,
            ts TIMESTAMP(3),
            proctime AS PROCTIME()
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'user_behaviors',
            'properties.bootstrap.servers' = 'localhost:9092',
            'properties.group.id' = 'flink-consumer-group',
            'scan.startup.mode' = 'earliest-offset',
            'format' = 'json',
            'json.fail-on-missing-field' = 'false',
            'json.ignore-parse-errors' = 'true'
        )
        """)
        logger.info("Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„± ì„±ê³µ")
    except Exception as e:
        logger.error(f"ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    # ì‹±í¬ í…Œì´ë¸” ì •ì˜
    try:
        logger.info("Kafka ì‹±í¬ í…Œì´ë¸” ìƒì„± ì‹œë„...")
        table_env.execute_sql("""
        CREATE TABLE kafka_sink (
            category STRING,
            behavior STRING,
            behavior_count BIGINT,
            update_time TIMESTAMP(3),
            PRIMARY KEY (category, behavior) NOT ENFORCED
        ) WITH (
            'connector' = 'kafka',
            'topic' = 'behavior_stats',
            'properties.bootstrap.servers' = 'localhost:9092',
            'key.format' = 'json',
            'value.format' = 'json',
            'properties.group.id' = 'flink-sink-group'
        )
        """)
        logger.info("Kafka ì‹±í¬ í…Œì´ë¸” ìƒì„± ì„±ê³µ")
    except Exception as e:
        logger.error(f"ì‹±í¬ í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    # ì‘ì—… ì œì¶œ
    try:
        logger.info("SQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹œë„...")
        stmt_set = table_env.create_statement_set()
        stmt_set.add_insert_sql("""
        INSERT INTO kafka_sink
        SELECT 
            category,
            behavior,
            COUNT(*) AS behavior_count,
            CURRENT_TIMESTAMP as update_time
        FROM kafka_source
        GROUP BY category, behavior
        """)
        
        # ì‘ì—… ì‹¤í–‰ ë° JobClient ê°€ì ¸ì˜¤ê¸°
        job_client = stmt_set.execute().get_job_client()
        
        if job_client:
            job_id = job_client.get_job_id()
            logger.info(f"ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—… ID: {job_id}")
            
            # ì‘ì—… ìƒíƒœ í™•ì¸
            monitor_job(job_client)
        else:
            logger.error("ì‘ì—… í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

def monitor_job(job_client):
    """ì‘ì—… ìƒíƒœì— ëŒ€í•œ ë¡œê·¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    try:
        # ì‘ì—… ìƒíƒœ í™•ì¸
        # Flink Job ìƒíƒœ ê°’
        # RUNNING    : Flink ì‘ì—…ì´ í˜„ì¬ ì‹¤í–‰ ì¤‘
        # FINISHED   : ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨
        # FAILED     : ì‘ì—… ì‹¤íŒ¨
        # CANCELED   : ì‘ì—…ì´ ì¤‘ë‹¨ë¨
        # RESTARTING : ì‘ì—…ì´ ì¬ì‹œì‘ ì¤‘
        
        job_status = job_client.get_job_status().result()

        logger.info(f"í˜„ì¬ ì‘ì—… ìƒíƒœ: {job_status}")
        
        # ìƒ˜í”Œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        logger.info("Kafka í† í”½ì— ìƒ˜í”Œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        logger.info("ìƒ˜í”Œ ë°ì´í„°ê°€ ì—†ë‹¤ë©´ kafka_producer.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        
        # ì‘ì—… ì‹¤í–‰ ì¤‘ ìƒíƒœ í™•ì¸
        print("\nì‘ì—… í™•ì¸ ì‹œì‘ (10ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸, Ctrl+Cë¡œ ì¢…ë£Œ)")
        for i in range(6):  # 60ì´ˆ ë™ì•ˆ í™•ì¸
            time.sleep(10)
            try:
                current_status = job_client.get_job_status().result()
                print(f"[{i+1}/6] í˜„ì¬ ì‘ì—… ìƒíƒœ: {current_status}")
                
                # ì„ íƒì : ì‘ì—… ë©”íŠ¸ë¦­ìŠ¤ í™•ì¸ (PyFlink APIê°€ ì§€ì›í•˜ëŠ” ê²½ìš°)
                # ì´ ë¶€ë¶„ì€ PyFlink ë²„ì „ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
                if hasattr(job_client, 'get_job_metrics'):
                    metrics = job_client.get_job_metrics()
                    print(f"ì‘ì—… ë©”íŠ¸ë¦­ìŠ¤: {metrics}")
            except Exception as e:
                print(f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("\ní™•ì¸ ì™„ë£Œ. ì‘ì—…ì€ ê³„ì† ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        print("ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        print("$KAFKA_HOME/bin/kafka-console-consumer.sh --topic behavior_stats --bootstrap-server localhost:9092 --from-beginning")
        
    except Exception as e:
        logger.error(f"ì‘ì—… í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == '__main__':
    main()
```

> **PyFlinkì˜ Table API**ë¥¼ ì´ìš©í•œ ì‹¤ì‹œê°„ ì§‘ê³„ ì‘ì—…

### â–¶ ì „ì²´ íë¦„ ìš”ì•½

```python
1. Flink í™˜ê²½ ì„¤ì • ë° Kafka ì»¤ë„¥í„° JAR ë¡œë”©
2. Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„±
3. Kafka ì‹±í¬ í…Œì´ë¸” ìƒì„±
4. SQL ì¿¼ë¦¬ ì •ì˜ ë° ì‹¤í–‰
5. Flink Job ì‹¤í–‰ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
```

<br>

3.4 ì£¼ìš” ì½”ë“œ ë¸”ë¡ ì„¤ëª…
---------------

### âœ… í™˜ê²½ ì„¤ì • ë° JAR ë“±ë¡

```python
env = StreamExecutionEnvironment.get_execution_environment()
env_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()
table_env = StreamTableEnvironment.create(env, environment_settings=env_settings)

# Kafka ì»¤ë„¥í„° JAR íŒŒì¼ ê²½ë¡œ ë“±ë¡
table_env.get_config().get_configuration().set_string(
    "pipeline.jars",
    "file:///ì ˆëŒ€ê²½ë¡œ/flink-sql-connector-kafka-3.3.0-1.20.jar"
)
```

*   `StreamExecutionEnvironment`: Flink ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ í™˜ê²½
    
*   `StreamTableEnvironment`: SQL ê¸°ë°˜ Table API ì‹¤í–‰ í™˜ê²½
    
*   Flinkì—ì„œ Kafkaë¥¼ SQLë¡œ ë‹¤ë£¨ë ¤ë©´ ì»¤ë„¥í„° JAR ë“±ë¡ì´ í•„ìš”í•¨
    

<br>

### âœ… Kafka ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„±

```sql
CREATE TABLE kafka_source (
    user_id STRING,
    item_id STRING,
    category STRING,
    behavior STRING,
    ts TIMESTAMP(3),
    proctime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'user_behaviors',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json',
    'scan.startup.mode' = 'earliest-offset',
    ...
)
```

*   `user_behaviors` í† í”½ì„ í†µí•´ ë“¤ì–´ì˜¤ëŠ” JSON ë©”ì‹œì§€ë¥¼ Flink í…Œì´ë¸”ë¡œ ë§¤í•‘
    
*   `ts`: ì´ë²¤íŠ¸ ë°œìƒ ì‹œê°
    
*   `proctime`: Flinkì˜ ì²˜ë¦¬ ì‹œê° â†’ ì‹œê°„ ê¸°ë°˜ ì—°ì‚°ì— ì‚¬ìš© ê°€ëŠ¥
    
*   `scan.startup.mode = 'earliest-offset'`: í† í”½ì˜ ì²˜ìŒë¶€í„° ë°ì´í„°ë¥¼ ì½ìŒ
    

<br>

### âœ… Kafka ì‹±í¬ í…Œì´ë¸” ìƒì„±

```sql
CREATE TABLE kafka_sink (
    category STRING,
    behavior STRING,
    behavior_count BIGINT,
    update_time TIMESTAMP(3),
    PRIMARY KEY (category, behavior) NOT ENFORCED
) WITH (
    'connector' = 'upsert-kafka',
    'topic' = 'behavior_stats',
    'key.format' = 'json',
    'value.format' = 'json',
    ...
)
```

*   Flinkê°€ ì²˜ë¦¬í•œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ Kafka í† í”½ì— ì „ì†¡í•˜ê¸° ìœ„í•œ í…Œì´ë¸”
    
*   `upsert-kafka`ëŠ” **í‚¤ ê¸°ë°˜ ë®ì–´ì“°ê¸°**ë¥¼ í—ˆìš©í•˜ëŠ” ì‹±í¬ ì»¤ë„¥í„°
    
*   `category + behavior` ì¡°í•©ì„ í‚¤ë¡œ ì‚¬ìš©í•˜ê³ , í•´ë‹¹ ì¡°í•©ì˜ ì§‘ê³„ ê°’ì„ Kafkaì— ì§€ì† ì—…ë°ì´íŠ¸
    

<br>

### âœ… SQL ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰

```sql
INSERT INTO kafka_sink
SELECT 
    category,
    behavior,
    COUNT(*) AS behavior_count,
    CURRENT_TIMESTAMP AS update_time
FROM kafka_source
GROUP BY category, behavior
```

*   ì‹¤ì‹œê°„ìœ¼ë¡œ ë“¤ì–´ì˜¤ëŠ” ì‚¬ìš©ì í–‰ë™ì„ **ì¹´í…Œê³ ë¦¬ + í–‰ë™ ìœ í˜•** ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„
    
*   `COUNT(*)`: í•´ë‹¹ ì¡°í•©ì˜ ë°œìƒ íšŸìˆ˜ ì§‘ê³„
    
*   `CURRENT_TIMESTAMP`: Flinkê°€ ì§‘ê³„í•œ ì‹œì  ê¸°ë¡
    

<br>

### âœ… ì‹¤í–‰ í›„ ëª¨ë‹ˆí„°ë§

```python
job_client = stmt_set.execute().get_job_client()
status = job_client.get_job_status().result()
```

*   `StatementSet`ì„ í†µí•´ SQL ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•˜ê³ , í•´ë‹¹ ì‘ì—…ì˜ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŒ
    
*   ì½˜ì†” ë¡œê·¸ ë˜ëŠ” Kafka consumerì—ì„œ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ
    

<br>

3.5 ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸
--------------

Kafka consumerë¥¼ ì‚¬ìš©í•˜ì—¬ Flinkê°€ ì „ì†¡í•˜ëŠ” ê²°ê³¼ ë©”ì‹œì§€ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë‹¤:

```bash
$KAFKA_HOME/bin/kafka-console-consumer.sh \
  --topic behavior_stats \
  --bootstrap-server localhost:9092 \
  --from-beginning
```

#### ì˜ˆì‹œ ì¶œë ¥:

```json
{"category":"books","behavior":"view","behavior_count":53,"update_time":"2025-04-21T14:15:00"}
```

<br>

3.6 ì‹¤ìŠµì˜ í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸
-----------------

*   Kafka â†’ Flink ì—°ê³„ë¥¼ ìœ„í•œ ì»¤ë„¥í„° ì„¤ì • ë° SQL ê¸°ë°˜ ì²˜ë¦¬ íë¦„ ì²´í—˜
    
*   ì‹¤ì‹œê°„ ì‚¬ìš©ì ë°ì´í„°ë¥¼ Flink SQLë¡œ ì§‘ê³„í•˜ëŠ” ë¡œì§ ì´í•´
    
*   Kafkaë¥¼ ì…ì¶œë ¥ ì–‘ìª½ì— í™œìš©í•˜ëŠ” ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì„¤ê³„
    
*   Flinkì˜ **Table API, StatementSet, proctime, upsert-kafka** ë“±ì˜ ì‹¤ì „ ì‚¬ìš©ë²• ì²´ë“
    

<br>
<br>

4\. Spark Structured Streamingì„ í™œìš©í•œ ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì§‘ê³„
==============================================


4.1 ì‹¤ìŠµ ëª©í‘œ
---------

ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” Kafkaì— ì €ì¥ëœ JSON ë©”ì‹œì§€ë¥¼ **Spark Structured Streaming**ìœ¼ë¡œ ìˆ˜ì‹ í•˜ê³ ,  
ê·¸ ì•ˆì— í¬í•¨ëœ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ **1ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ì§‘ê³„**ë¥¼ ìˆ˜í–‰í•œë‹¤.

ì´ ì‹¤ìŠµì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ëª©ì ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤:

*   Kafka + Spark ì—°ë™ ê²½í—˜
    
*   íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ì§‘ê³„ (event time)
    
*   Structured Streamingì˜ **Watermark** ê°œë… ì‹¤ìŠµ
    
*   Flinkì™€ Spark ê°„ì˜ ì°¨ì´ì  ë¹„êµ
    

<br>

4.2 ì‹¤ìŠµ íë¦„ ìš”ì•½
------------

```mermaid
graph TD
    A[Kafka Topic: test-topic] --> B[Spark Structured Streaming]
    B --> C[ì½˜ì†” ì¶œë ¥ (1ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ë©”ì‹œì§€ ìˆ˜)]
```

*   Kafkaë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ì½ìŒ
    
*   ê° ë©”ì‹œì§€ì˜ timestamp í•„ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœˆë„ìš° ì§‘ê³„
    
*   ì§‘ê³„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    

<br>

4.3 Kafka ë©”ì‹œì§€ ì˜ˆì‹œ (Producer ì½”ë“œ `spark_producer.py`ì—ì„œ ìƒì„±)
-------------------------------------------------------

```json
{
  "id": 12,
  "message": "í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ 12",
  "timestamp": "2025-04-21 14:08:35"
}
```

*   SparkëŠ” ì´ ë©”ì‹œì§€ì˜ `timestamp` ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„ë¥¼ ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.
    
### ğŸ“„ ì „ì²´ êµ¬ì¡°

```python
from kafka import KafkaProducer
import json
import time
from datetime import datetime, timedelta
import random

# Kafka Producer ìƒì„±
producer = KafkaProducer(
    bootstrap_servers=['localhost:9092'],
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# ê¸°ì¤€ ì‹œê°„ (í˜„ì¬ ì‹œê° ê¸°ì¤€ -5ë¶„)
base_time = datetime.now() - timedelta(minutes=5)

# ë©”ì‹œì§€ ì „ì†¡
for i in range(30):
    # 0~4ë¶„ ì‚¬ì´ì˜ ì„ì˜ ì‹œì  ìƒì„± (1ë¶„ ë‹¨ìœ„ ì§‘ê³„ë¥¼ ìœ„í•´)
    offset_minutes = random.randint(0, 4)
    offset_seconds = random.randint(0, 59)

    msg_time = base_time + timedelta(minutes=offset_minutes, seconds=offset_seconds)
    
    message = {
        'id': i,
        'message': f'í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ {i}',
        'timestamp': msg_time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    # Kafkaë¡œ ë©”ì‹œì§€ ì „ì†¡
    producer.send('test-topic', message)
    print(f'ì „ì†¡ëœ ë©”ì‹œì§€: {message}')
    
    time.sleep(0.5)  # ë„ˆë¬´ ë¹ ë¥´ê²Œ ë³´ë‚´ì§€ ì•Šë„ë¡ ì‚´ì§ í…€ ì£¼ê¸°

producer.flush()
```

<br>

4.4 í•µì‹¬ ì½”ë“œ ì„¤ëª…: `spark_kafka_example.py`
--------------------------------------

### ğŸ“„ ì „ì²´ êµ¬ì¡°

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import findspark

# ë¡œì»¬ Python í™˜ê²½ì—ì„œ Sparkë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì´ˆê¸°í™”
findspark.init()

# Spark ì„¸ì…˜ ìƒì„± í•¨ìˆ˜
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ë¦„ ì§€ì •
# Kafka ì—°ë™ìš© íŒ¨í‚¤ì§€ ìë™ ë‹¤ìš´ë¡œë“œ
def create_spark_session():
    return SparkSession.builder \
        .appName("KafkaSparkStreaming") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
        .getOrCreate()

# Kafka ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜
def process_kafka_stream():
    # Spark ì„¸ì…˜ ìƒì„±
    spark = create_spark_session()
    
    # Kafkaì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ì½ê¸°
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì½ê¸° ì‹œì‘
    # ë°ì´í„° ì†ŒìŠ¤ í˜•ì‹ ì§€ì •
    # Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
    # êµ¬ë…í•  Kafka í† í”½ ì´ë¦„
    # ê°€ì¥ ë§ˆì§€ë§‰ ì˜¤í”„ì…‹ë¶€í„° ì½ê¸° ì‹œì‘
    df = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "test-topic") \
        .option("startingOffsets", "latest") \
        .load()
    
    # Kafka ë©”ì‹œì§€ì˜ valueëŠ” binary í˜•íƒœ â†’ ë¬¸ìì—´ë¡œ ë³€í™˜
    value_df = df.selectExpr("CAST(value AS STRING)")

    # Kafka ë©”ì‹œì§€ ì•ˆì˜ JSONì„ íŒŒì‹±í•˜ê¸° ìœ„í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜
    # ì´ë²¤íŠ¸ ë°œìƒ ì‹œê°
    # ë©”ì‹œì§€ ë³¸ë¬¸
    schema = StructType([
        StructField("timestamp", TimestampType(), True),
        StructField("message", StringType(), True)
    ])
    
    # JSON ë¬¸ìì—´ì„ êµ¬ì¡°í™”ëœ ì»¬ëŸ¼ìœ¼ë¡œ íŒŒì‹±
    # JSON íŒŒì‹± í›„ "data" ì»¬ëŸ¼ìœ¼ë¡œ ê°ì‹¸ê¸°
    # ì»¬ëŸ¼ í¼ì¹˜ê¸° (timestamp, message ë“±ìœ¼ë¡œ ë¶„ë¦¬)
    parsed_df = value_df.select(
        from_json(col("value"), schema).alias("data")
    ).select("data.*")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€ìœ¼ë¡œ 1ë¶„ ë‹¨ìœ„ ìœˆë„ìš° ì§‘ê³„
    # 1ë¶„ ì§€ì—°ê¹Œì§€ í—ˆìš© (ì´ë²¤íŠ¸ íƒ€ì„ ê¸°ë°˜)
    # 1ë¶„ ë‹¨ìœ„ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ ìœˆë„ìš° ë¶„í• 
    result_df = parsed_df \
        .withWatermark("timestamp", "1 minute") \
        .groupBy(window("timestamp", "1 minute")) \
        .count()  # ê° ìœˆë„ìš°ë³„ ë©”ì‹œì§€ ìˆ˜ ê³„ì‚°
    
    # ì‹¤ì‹œê°„ ì§‘ê³„ ê²°ê³¼ë¥¼ ì½˜ì†”ì— ì¶œë ¥
    # ìŠ¤íŠ¸ë¦¼ ê²°ê³¼ ì“°ê¸° ì‹œì‘
    # ì „ì²´ ìœˆë„ìš° ê²°ê³¼ë¥¼ ê³„ì† ê°±ì‹ 
    # ì½˜ì†”ë¡œ ì¶œë ¥
    # ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì‹œì‘
    query = result_df \
        .writeStream \
        .outputMode("complete") \
        .format("console") \
        .start()

    # ìŠ¤íŠ¸ë¦¬ë° ì‘ì—…ì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    query.awaitTermination()

# ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰
if __name__ == "__main__":
    process_kafka_stream()
```

### âœ… SparkSession ì„¤ì •

```python
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()
```

*   Kafkaë¥¼ Sparkì™€ ì—°ë™í•˜ë ¤ë©´ ìœ„ì™€ ê°™ì€ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì •í•´ì•¼ í•œë‹¤.
    
*   PySparkì—ì„œ Kafka í† í”½ì„ ìŠ¤íŠ¸ë¦¼ ì†ŒìŠ¤ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤.
    

<br>

### âœ… Kafkaì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹ 

```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "test-topic") \
    .option("startingOffsets", "latest") \
    .load()
```

*   `test-topic`ìœ¼ë¡œë¶€í„° ì‹¤ì‹œê°„ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ 
    
*   Kafka ë©”ì‹œì§€ì˜ `key`, `value`, `timestamp` ë“±ì´ binary í˜•íƒœë¡œ ë“¤ì–´ì˜´
    

<br>

### âœ… JSON ë¬¸ìì—´ íŒŒì‹± ë° ìŠ¤í‚¤ë§ˆ ì ìš©

```python
from pyspark.sql.types import StructType, StructField, TimestampType, StringType

schema = StructType([
    StructField("timestamp", TimestampType(), True),
    StructField("message", StringType(), True)
])

parsed_df = df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")
```

*   Kafka ë©”ì‹œì§€ì˜ `value`ëŠ” JSON ë¬¸ìì—´ â†’ `CAST(value AS STRING)` í•„ìš”
    
*   `from_json()`ì„ í†µí•´ êµ¬ì¡°í™”ëœ DataFrameìœ¼ë¡œ ë³€í™˜
    
*   `"data.*"`ë¥¼ í†µí•´ ë‚´ë¶€ í•„ë“œë¥¼ í¼ì¹¨ (message, timestamp)
    

<br>

### âœ… íƒ€ì„ìœˆë„ìš° + Watermark ì§‘ê³„ ìˆ˜í–‰

```python
result_df = parsed_df \
    .withWatermark("timestamp", "1 minute") \
    .groupBy(window("timestamp", "1 minute")) \
    .count()
```

*   `withWatermark()`ëŠ” ëŠ¦ê²Œ ë„ì°©í•˜ëŠ” ë°ì´í„°ë¥¼ ì–´ëŠ ì •ë„ê¹Œì§€ ìˆ˜ìš©í• ì§€ ê²°ì •
    
*   `window("timestamp", "1 minute")`: íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 1ë¶„ ë‹¨ìœ„ ì§‘ê³„
    
*   `count()`: ê° ì‹œê°„ êµ¬ê°„ë§ˆë‹¤ ë©”ì‹œì§€ ìˆ˜ë¥¼ ì„¸ì–´ì¤Œ
    

<br>

### âœ… ì½˜ì†”ë¡œ ì‹¤ì‹œê°„ ì¶œë ¥

```python
query = result_df.writeStream \
    .outputMode("complete") \
    .format("console") \
    .start()

query.awaitTermination()
```

*   `complete` ëª¨ë“œëŠ” ì§‘ê³„ëœ ì „ì²´ ìœˆë„ìš° ê²°ê³¼ë¥¼ ì§€ì†ì ìœ¼ë¡œ ì¶œë ¥
    
*   ì½˜ì†”ì— ì¶œë ¥ëœ ë©”ì‹œì§€ë¥¼ í†µí•´ ì§‘ê³„ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŒ
    

<br>

4.5 ì‹¤í–‰ ì˜ˆì‹œ ë° ê²°ê³¼ í•´ì„
-----------------

### âœ… Producer ì‹¤í–‰ (`spark_producer.py`)

```bash
$ python spark_producer.py
ì „ì†¡ëœ ë©”ì‹œì§€: {'id': 3, 'message': 'í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ 3', 'timestamp': '2025-04-21 14:08:35'}
...
```

### âœ… Structured Streaming ì‹¤í–‰

```bash
$ python spark_kafka_example.py
+------------------------------------------+-----+
|window                                    |count|
+------------------------------------------+-----+
|{2025-04-21 14:08:00, 2025-04-21 14:09:00}|  7  |
|{2025-04-21 14:09:00, 2025-04-21 14:10:00}|  6  |
+------------------------------------------+-----+
```

*   ì¶œë ¥ì—ì„œ `"window"`ëŠ” ì§‘ê³„ ì‹œê°„ êµ¬ê°„
    
*   `"count"`ëŠ” í•´ë‹¹ ì‹œê°„ëŒ€ì— ìˆ˜ì‹ ëœ ë©”ì‹œì§€ ìˆ˜
    

<br>

4.6 Flinkì™€ ë¹„êµí•˜ë©° ì´í•´í•˜ê¸°
--------------------

| í•­ëª©        | Spark Structured Streaming    | Apache Flink                        |
| ----------- | ----------------------------- | ----------------------------------- |
| ì²˜ë¦¬ ë°©ì‹   | ë§ˆì´í¬ë¡œ ë°°ì¹˜                 | ì´ë²¤íŠ¸ ë‹¨ìœ„ ì²˜ë¦¬                    |
| ê¸°ë³¸ API    | DataFrame, SQL                | Table API, DataStream API           |
| ì§‘ê³„ ë°©ì‹   | `window()`, `withWatermark()` | `GROUP BY`, `PROCTIME`, `WATERMARK` |
| ì‹¤ìŠµ ë‚œì´ë„ | ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€               | ì„¸ë°€í•œ ì œì–´ ê°€ëŠ¥ (ë³µì¡ë„ â†‘)         |

<br>

4.7 í•™ìŠµ í¬ì¸íŠ¸
----------

*   Kafka ë©”ì‹œì§€ë¥¼ PySparkì—ì„œ ì‹¤ì‹œê°„ ìˆ˜ì‹ í•˜ëŠ” ë²•
    
*   JSON êµ¬ì¡°ì˜ ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ë¶„ì„ ëŒ€ìƒìœ¼ë¡œ ë§Œë“œëŠ” ì‹¤ì „ ê²½í—˜
    
*   `withWatermark()`ë¥¼ ì´ìš©í•œ event time ê¸°ë°˜ ì§‘ê³„
    
*   Flinkì™€ Sparkì˜ ì²˜ë¦¬ íŒ¨ëŸ¬ë‹¤ì„ ì°¨ì´ ì´í•´
    

<br>
<br>

5\. PyFlink Table APIë¥¼ í™œìš©í•œ ì •ì  ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬
=======================================


5.1 ì‹¤ìŠµ ëª©ì 
---------

ë³¸ ì‹¤ìŠµì—ì„œëŠ” PyFlinkë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤:

*   CSVë¡œ ì €ì¥ëœ ì •ì  ë°ì´í„°ë¥¼ ì½ëŠ”ë‹¤.
    
*   íŒë§¤ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ë¡œ ì§‘ê³„í•œë‹¤.
    
*   ì§‘ê³„ ê²°ê³¼ë¥¼ ë˜ ë‹¤ë¥¸ CSV íŒŒì¼ë¡œ ì¶œë ¥í•œë‹¤.
    
*   Pandasë¥¼ í†µí•´ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤.
    

ì´ ì‹¤ìŠµì„ í†µí•´ Flink Table APIì˜ **SQL ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬ ëŠ¥ë ¥**ê³¼  
íŒŒì¼ ê¸°ë°˜ ETL íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë°©ì‹ì„ ì§ì ‘ ê²½í—˜í•  ìˆ˜ ìˆë‹¤.

<br>

5.2 ì‹¤ìŠµ íë¦„ ìš”ì•½
------------

```mermaid
graph TD
    A[CSV ì†ŒìŠ¤ ë°ì´í„° (sales_data.csv)] --> B[Flink Table API ë°°ì¹˜ ì²˜ë¦¬]
    B --> C[ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„ ê²°ê³¼ CSV (sales_summary)]
    C --> D[Pandas ë¡œë”© ë° ì¶œë ¥]
```

<br>

5.3 ìƒ˜í”Œ ì…ë ¥ ë°ì´í„° êµ¬ì¡° (`create_sample_csv()` ìë™ ìƒì„±)
----------------------------------------------

CSV ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤:

| ì»¬ëŸ¼ëª…            | ì„¤ëª…                    |
| ----------------- | ----------------------- |
| product\_id       | ìƒí’ˆ ID                 |
| category          | ìƒí’ˆ ì¹´í…Œê³ ë¦¬           |
| price             | ë‹¨ê°€                    |
| quantity          | ìˆ˜ëŸ‰                    |
| transaction\_date | ê±°ë˜ì¼ì (`YYYY-MM-DD`) |

ì˜ˆì‹œ:

```
P321,electronics,899.99,3,2024-02-21
P514,accessories,45.20,1,2024-03-12
```

<br>

5.4 í•µì‹¬ ì½”ë“œ ì„¤ëª…: `pyflink_batch_example.py`
----------------------------------------

### ğŸ“„ ì „ì²´ êµ¬ì¡°

```python
from pyflink.table import EnvironmentSettings, TableEnvironment
from pyflink.table.expressions import col
import tempfile
import os
import pandas as pd
import numpy as np
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    logger.info("Flink ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í…Œì´ë¸” í™˜ê²½ ì„¤ì •
    env_settings = EnvironmentSettings.in_batch_mode()
    table_env = TableEnvironment.create(env_settings)
    
    # ì‹¤í–‰ ì„¤ì •
    config = table_env.get_config().get_configuration()    
    config.set_string("parallelism.default", "1")
    config.set_string("pipeline.auto-watermark-interval", "0")
    
    # ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •
    input_path = create_sample_csv()
    output_dir = os.path.join(os.getcwd(), 'flink_output')  # í˜„ì¬ ë””ë ‰í† ë¦¬ë¡œ ë³€ê²½
    
    logger.info(f"ì…ë ¥ íŒŒì¼ ê²½ë¡œ: {input_path}")
    logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ: {output_dir}")
    
    # ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì‚­ì œ
    if os.path.exists(output_dir):
        import shutil
        shutil.rmtree(output_dir)
        logger.info("ê¸°ì¡´ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì‚­ì œë¨")
    
    # ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„±
    source_ddl = f"""
    CREATE TABLE sales (
        product_id STRING,
        category STRING,
        price DOUBLE,
        quantity INT,
        transaction_date STRING
    ) WITH (
        'connector' = 'filesystem',
        'path' = '{input_path}',
        'format' = 'csv',
        'csv.field-delimiter' = ',',
        'csv.ignore-parse-errors' = 'true'
    )
    """
    logger.info("ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„± ì‹œì‘")
    table_env.execute_sql(source_ddl)
    logger.info("ì†ŒìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
    # ì‹±í¬ í…Œì´ë¸” ìƒì„±
    sink_ddl = f"""
    CREATE TABLE sales_summary (
        category STRING,
        total_revenue DOUBLE,
        avg_price DOUBLE,
        total_quantity BIGINT,
        transaction_count BIGINT
    ) WITH (
        'connector' = 'filesystem',
        'path' = '{output_dir}',
        'format' = 'csv',
        'csv.field-delimiter' = ',',
        'csv.quote-character' = '"',
        'sink.rolling-policy.file-size' = '1MB',
        'sink.rolling-policy.rollover-interval' = '0',
        'sink.rolling-policy.check-interval' = '1s'
    )
    """
    logger.info("ì‹±í¬ í…Œì´ë¸” ìƒì„± ì‹œì‘")
    table_env.execute_sql(sink_ddl)
    logger.info("ì‹±í¬ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    
    # SQL ì¿¼ë¦¬ ì‹¤í–‰
    logger.info("ë°ì´í„° ì²˜ë¦¬ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘")
    result = table_env.execute_sql("""
    INSERT INTO sales_summary
    SELECT 
        category,
        SUM(price * quantity) AS total_revenue,
        AVG(price) AS avg_price,
        SUM(quantity) AS total_quantity,
        COUNT(*) AS transaction_count
    FROM sales
    GROUP BY category
    """)
    
    logger.info(f"ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼: {result.get_job_client().get_job_status()}")
    print(f"ë°°ì¹˜ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” {output_dir} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ê²°ê³¼ í™•ì¸
    try:
        import time
        time.sleep(2)
        result_files = [f for f in os.listdir(output_dir) if f.startswith('part-')]
        logger.info(f"ë°œê²¬ëœ ê²°ê³¼ íŒŒì¼ë“¤: {result_files}")
        if not result_files:
            raise FileNotFoundError("ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        result_file = os.path.join(output_dir, result_files[0])
        result_df = pd.read_csv(result_file, header=None,
            names=['category', 'total_revenue', 'avg_price', 'total_quantity', 'transaction_count'])
        print("\n===== ì²˜ë¦¬ ê²°ê³¼ =====")
        print(result_df)
    except Exception as e:
        logger.error(f"ê²°ê³¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ê²°ê³¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ì¶œë ¥ í´ë”ë¥¼ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.")

def create_sample_csv():
    """ìƒ˜í”Œ íŒë§¤ ë°ì´í„° ìƒì„±"""
    temp_file = os.path.join(os.getcwd(), 'sales_data.csv')  # í˜„ì¬ ë””ë ‰í† ë¦¬ì— ì €ì¥
    np.random.seed(42)
    products = ['laptop', 'smartphone', 'tablet', 'monitor', 'keyboard']
    categories = ['electronics', 'accessories', 'peripherals']
    data = []
    for i in range(1000):
        product_id = f"P{np.random.randint(100, 999)}"
        category = np.random.choice(categories)
        price = round(np.random.uniform(10.0, 1000.0), 2)
        quantity = np.random.randint(1, 10)
        date = f"2024-{np.random.randint(1, 13):02d}-{np.random.randint(1, 29):02d}"
        data.append(f"{product_id},{category},{price},{quantity},{date}")
    with open(temp_file, 'w') as f:
        f.write('\n'.join(data))
    print(f"ìƒ˜í”Œ ë°ì´í„°ê°€ {temp_file}ì— ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return temp_file

if __name__ == '__main__':
    main()
```

### âœ… Flink ë°°ì¹˜ ëª¨ë“œ í™˜ê²½ ì„¤ì •

```python
env_settings = EnvironmentSettings.in_batch_mode()
table_env = TableEnvironment.create(env_settings)

table_env.get_config().get_configuration().set_string("parallelism.default", "1")
```

*   `in_batch_mode()`: Table APIë¥¼ **ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë“œ**ë¡œ ì„¤ì •
    
*   ë³‘ë ¬ë„(parallelism) ê¸°ë³¸ê°’ì„ 1ë¡œ ì„¤ì •í•˜ì—¬ ë‹¨ìˆœ ì‹¤í–‰ í™˜ê²½ êµ¬ì„±
    

<br>

### âœ… CSV ì†ŒìŠ¤ í…Œì´ë¸” ì •ì˜

```sql
CREATE TABLE sales (
    product_id STRING,
    category STRING,
    price DOUBLE,
    quantity INT,
    transaction_date STRING
) WITH (
    'connector' = 'filesystem',
    'path' = '{input_path}',
    'format' = 'csv',
    'csv.field-delimiter' = ','
)
```

*   CSV í˜•ì‹ì˜ íŒŒì¼ì„ Flink í…Œì´ë¸”ë¡œ ì¸ì‹
    
*   í•„ë“œ íƒ€ì…ì„ SQL ìŠ¤í‚¤ë§ˆë¡œ ëª…ì‹œ
    
*   `'connector' = 'filesystem'` ì‚¬ìš© â†’ ë¡œì»¬ íŒŒì¼ë„ ë°ì´í„° ì†ŒìŠ¤ë¡œ í™œìš© ê°€ëŠ¥
    

<br>

### âœ… CSV ì‹±í¬ í…Œì´ë¸” ì •ì˜

```sql
CREATE TABLE sales_summary (
    category STRING,
    total_revenue DOUBLE,
    avg_price DOUBLE,
    total_quantity BIGINT,
    transaction_count BIGINT
) WITH (
    'connector' = 'filesystem',
    'path' = '{output_dir}',
    'format' = 'csv',
    'csv.field-delimiter' = ','
)
```

*   ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ ì§‘ê³„ ê²°ê³¼ë¥¼ ì €ì¥í•  ì‹±í¬ í…Œì´ë¸”
    
*   ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ëŠ” ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— `part-*.csv` í˜•íƒœë¡œ ì €ì¥ë¨
    

<br>

### âœ… SQL ì§‘ê³„ ì¿¼ë¦¬ ì‹¤í–‰

```sql
INSERT INTO sales_summary
SELECT 
    category,
    SUM(price * quantity) AS total_revenue,
    AVG(price) AS avg_price,
    SUM(quantity) AS total_quantity,
    COUNT(*) AS transaction_count
FROM sales
GROUP BY category
```

*   ì¹´í…Œê³ ë¦¬ë³„ ë§¤ì¶œ(total revenue), í‰ê·  ê°€ê²©, ì´ ìˆ˜ëŸ‰, ê±°ë˜ ê±´ìˆ˜ ê³„ì‚°
    
*   SQL ë¬¸ë²•ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì—ì„œ **Flink Table APIì˜ ë†’ì€ ì¶”ìƒí™” ìˆ˜ì¤€**ì„ ì²´í—˜í•  ìˆ˜ ìˆìŒ
    

<br>

### âœ… ê²°ê³¼ íŒŒì¼ ë¡œë”© ë° Pandas ì¶œë ¥

```python
result_df = pd.read_csv(result_file, header=None, names=[
    'category', 'total_revenue', 'avg_price', 'total_quantity', 'transaction_count'
])
print(result_df)
```

*   FlinkëŠ” ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° ë•Œë¬¸ì—, Python í™˜ê²½ì—ì„œ Pandasë¥¼ í†µí•´ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ
    
*   ETL ì‹¤ë¬´ì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆëŠ” â€œFlink â†’ CSV â†’ í›„ì²˜ë¦¬â€ì˜ êµ¬ì¡°ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆìŒ
    

<br>

5.5 ì‹¤í–‰ ê²°ê³¼ ì˜ˆì‹œ
------------

```plaintext
        category  total_revenue  avg_price  total_quantity  transaction_count
0    electronics       143009.1     505.22             310                105
1     peripherals        90231.8     413.75             281                 95
2    accessories        48123.4     231.60             213                100
```

<br>

5.6 ì‹¤ìŠµ ìš”ì•½ ë° í•™ìŠµ í¬ì¸íŠ¸
------------------

*   PyFlinkë¥¼ í†µí•´ ì •ì  ë°ì´í„°ë¥¼ SQLë¡œ ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ êµ¬í˜„
    
*   FileSystem ì»¤ë„¥í„°ë¥¼ í™œìš©í•˜ì—¬ CSVë¥¼ ë°ì´í„° ì†ŒìŠ¤/ì‹±í¬ë¡œ ì‚¬ìš©
    
*   Table API ê¸°ë°˜ì˜ ì§‘ê³„, ë³€í™˜, ê·¸ë£¹ ì—°ì‚° í•™ìŠµ
    
*   Pandas ì—°ê³„ë¡œ ì‚¬í›„ ë¶„ì„ ê²½í—˜
    

<br>
<br>

6\. Kafka, Flink, Spark í†µí•© ì‹¤ìŠµ ì´ì •ë¦¬
=================================


6.1 ì „ì²´ ì‹¤ìŠµ íë¦„ ìš”ì•½
---------------

ì´ë²ˆ ì‹¤ìŠµì€ **ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸**ì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ëŠ” ê²½í—˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆë‹¤.  
Kafkaë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ Flink, Spark, ê·¸ë¦¬ê³  PyFlinkì˜ Table APIë¥¼ ì—°ê³„í•˜ë©°,  
ì‹¤ì œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì „ë°˜ì ì¸ íë¦„ì„ ì²´ê³„ì ìœ¼ë¡œ ë‹¤ë£¨ì—ˆë‹¤.

<br>

### ğŸ§­ ì‹¤ìŠµ íë¦„ í†µí•© ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    A[Kafka Producer (flink_producer.py)] --> B[Kafka Topic: user_behaviors]
    B --> C[Flink ì²˜ë¦¬ (kafka_flink_example.py)]
    C --> D[Kafka Topic: behavior_stats]
    E[Kafka Producer (spark_producer.py)] --> F[Kafka Topic: test-topic]
    F --> G[Spark Streaming ì§‘ê³„ (spark_kafka_example.py)]
    H[CSV íŒŒì¼ ìƒì„±] --> I[PyFlink ë°°ì¹˜ ì²˜ë¦¬ (pyflink_batch_example.py)]
    I --> J[CSV ê²°ê³¼ íŒŒì¼ â†’ Pandas ë¡œë”©]
```

<br>

6.2 ê° ì‹¤ìŠµ êµ¬ì„±ì˜ ì˜ë¯¸ì™€ í•µì‹¬ í¬ì¸íŠ¸
-----------------------

| ì‹¤ìŠµ êµ¬ì„±                  | ëª©ì                                       | ì£¼ìš” ê¸°ìˆ               | í•™ìŠµ í¬ì¸íŠ¸                              |
| -------------------------- | ----------------------------------------- | ---------------------- | ---------------------------------------- |
| `flink_producer.py`        | Kafkaì— ì‹¤ì‹œê°„ ì´ë²¤íŠ¸ ì „ì†¡                | KafkaProducer (Python) | JSON ë©”ì‹œì§€ êµ¬ì¡°, ì‹œë®¬ë ˆì´ì…˜ ì´ë²¤íŠ¸ ìƒì„± |
| `kafka_flink_example.py`   | Flinkë¡œ Kafka ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì‹¤ì‹œê°„ ì§‘ê³„ | Flink Table API + SQL  | Kafka-Connector, proctime, upsert-kafka  |
| `spark_producer.py`        | íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨ ë©”ì‹œì§€ ì „ì†¡               | KafkaProducer (Python) | Event Time ê¸°ë°˜ ì§‘ê³„ìš© ë°ì´í„° ìƒì„±       |
| `spark_kafka_example.py`   | Spark Structured Streamingìœ¼ë¡œ Kafka ì§‘ê³„ | PySpark + Kafka ì—°ë™   | withWatermark(), window(), from\_json()  |
| `pyflink_batch_example.py` | ì •ì  CSV ë°ì´í„°ë¥¼ ë°°ì¹˜ ì§‘ê³„               | PyFlink Table API      | FileSystem Connector, SQL ê¸°ë°˜ ETL       |

<br>

6.3 ê¸°ìˆ ë³„ íŠ¹ì§• ì •ë¦¬
-------------

| ê¸°ìˆ                 | í•µì‹¬ ê°œë…                   | ì¥ì                         | ì‚¬ìš© ì‹œì                                |
| ------------------- | --------------------------- | --------------------------- | --------------------------------------- |
| **Kafka**           | ë©”ì‹œì§€ í (ë°ì´í„° ë²„ìŠ¤)     | ë†’ì€ ì²˜ë¦¬ëŸ‰, ì¥ì•  ë³µì›ë ¥    | ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì†¡              |
| **Flink**           | ì´ë²¤íŠ¸ ê¸°ë°˜ ì‹¤ì‹œê°„ ì²˜ë¦¬     | ë‚®ì€ ì§€ì—°, ì •ë°€í•œ ì‹œê°„ ì œì–´ | ì‹¤ì‹œê°„ ë¶„ì„, íŒ¨í„´ íƒì§€                  |
| **Spark**           | ë§ˆì´í¬ë¡œ ë°°ì¹˜ ê¸°ë°˜ ì²˜ë¦¬     | ìœ ì—°í•œ API, ì‰¬ìš´ ì‚¬ìš©ì„±     | ë¡œê·¸ ë¶„ì„, ì§€ì—° í—ˆìš© ê°€ëŠ¥í•œ ì‹¤ì‹œê°„ ì§‘ê³„ |
| **PyFlink (Batch)** | ì •ì  ë°ì´í„° ì²˜ë¦¬ìš© SQL ì—”ì§„ | SQLê³¼ DataFrame í˜¼í•©        | íŒŒì¼ ê¸°ë°˜ ì§‘ê³„, ì‚¬í›„ ë°ì´í„° ì²˜ë¦¬        |

<br>

6.4 ì‹¤ë¬´ì—ì„œì˜ í™œìš© ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ
--------------------

| ì‹œë‚˜ë¦¬ì˜¤                     | ì í•©í•œ ê¸°ìˆ  ì¡°í•©      | ì„¤ëª…                                  |
| ---------------------------- | --------------------- | ------------------------------------- |
| ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ | Kafka + Flink         | ì‹¤ì‹œê°„ í–‰ë™ ë¶„ì„ ë° í”¼ë“œë°± ì œê³µ       |
| ë¼ì´ë¸Œ ë°©ì†¡ íŠ¸ë˜í”½ ëª¨ë‹ˆí„°ë§  | Kafka + Spark         | ë¶„ ë‹¨ìœ„ë¡œ ìœ ì…ëŸ‰ ì§‘ê³„, ëŒ€ì‹œë³´ë“œ ì œê³µ  |
| ì›”ë³„ íŒë§¤ ë°ì´í„° ì§‘ê³„ ë¦¬í¬íŠ¸ | PyFlink (Batch)       | ì •ì  CSV íŒŒì¼ì„ SQLë¡œ ì •ë¦¬, ê²°ê³¼ ì €ì¥ |
| ì•Œë¦¼ ì„œë¹„ìŠ¤                  | Kafka + Flink + Kafka | íŠ¹ì • í–‰ë™ ê°ì§€ â†’ ë©”ì‹œì§€ ë°œì†¡ íŠ¸ë¦¬ê±°   |
