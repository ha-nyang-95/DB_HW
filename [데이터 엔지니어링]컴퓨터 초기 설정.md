
# Docker Desktop (WSL 2 기반) 설치 및 Ubuntu 설정 가이드 (for Windows)

---

## 0. Docker란?

- Docker는 애플리케이션을 컨테이너(container)라는 독립된 환경에서 실행할 수 있도록 도와주는 플랫폼입니다.
- 컨테이너는 일종의 작은 가상 머신처럼 동작하지만, 훨씬 가볍고 빠릅니다.
- 예를 들어, Python 웹 서버, 데이터베이스, 머신러닝 모델 등을 Docker로 실행하면
  - 다른 컴퓨터에서도 동일한 환경으로 실행 가능
  - 설치/설정 충돌 없이 독립적으로 운영 가능
- 개발, 테스트, 배포 자동화에 널리 사용됩니다.

> Docker는 "환경 구성 및 설치 스트레스 없이 실행되는 환경"를 만들 수 있게 해줍니다. --> airflow, elasticsearch는 docker-compose 사용 예정

---

## 1. WSL 2 기반으로 Docker를 사용하는 이유

- Windows에서 Docker를 실행하려면 리눅스 커널이 필요합니다.
- WSL 2 (Windows Subsystem for Linux 2)는 Windows에서 리눅스 환경을 가볍게 사용할 수 있게 해주는 기능입니다.
- Docker Desktop은 WSL 2와 연동되어 빠르고 안정적인 컨테이너 실행 환경을 제공합니다.
- Hyper-V 기반보다 더 유연하며, Windows Home 버전에서도 사용할 수 있습니다.

---

## 2. 필수 Windows 기능 활성화 (이미 설정된 경우 생략)

```powershell
dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart
dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart
```

- 이미 설정한 경우 재실행 불필요
- WSL 2로 설정되어 있으면 OK
```bash
wsl --set-default-version 2
```

---

## 3. 기존 Ubuntu WSL 환경 삭제해야 하는 경우우

### 1) 현재 설치된 WSL 목록 확인

```bash
wsl --list --verbose
```

### 2) Ubuntu 종료 및 삭제

```bash
wsl --terminate Ubuntu-24.04
wsl --unregister Ubuntu-24.04
```

```powershell
Get-AppxPackage *Ubuntu* | Remove-AppxPackage
```

---

## 4. Ubuntu 재설치

### Microsoft Store에서 설치

1. Microsoft Store 실행  
2. `Ubuntu` 검색 → 원하는 버전 선택 (예: Ubuntu 22.04 LTS, 24.04 등 -> 우리는 24.04.1 LTS로 통일)  
3. 설치 후 실행하여 사용자 이름(ssafy), 비밀번호 설정(ssafy)


> 설치 후 기본 배포판 설정
```powershell
wsl --set-default Ubuntu-24.04
```

---

## 5. Docker Desktop 설치 (기존 유지 시 생략 가능)

- [공식 다운로드 링크](https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe)
- 설치 중 "Use WSL 2 instead of Hyper-V" 선택
- 설치 완료 후 Docker Desktop 실행

---

## 6. Docker Desktop과 Ubuntu 연동 재설정

### Docker Desktop → Settings → Resources → WSL Integration

- 새로 설치한 Ubuntu가 목록에 보이면 **체크박스 활성화**
- 보이지 않으면 Ubuntu를 한 번 실행 후 Docker Desktop을 재시작

---

## 7. Ubuntu에서 Docker 정상 연동 확인

```bash
docker --version
docker run hello-world
```

### 만약 `docker` 명령이 안 된다면:

```bash
sudo apt update
sudo apt install docker-cli -y
```

> `docker engine`은 설치하지 말 것!  
Docker Desktop이 엔진 역할을 하므로 Ubuntu에서는 CLI만 필요

---

## 8. VS Code에서 WSL Ubuntu로 개발 연결 (옵션)

1. VS Code 실행  
2. `Remote - WSL` 확장 설치  
3. `Ctrl+Shift+P` → `Remote-WSL: New Window` 실행  
4. Ubuntu 선택 → 리눅스 환경에서 코드 편집 가능

---

## 9. Docker 기본 명령어 요약

```bash
docker ps            # 실행 중인 컨테이너
docker ps -a         # 모든 컨테이너
docker images        # 이미지 목록
docker rm [ID]       # 컨테이너 삭제
docker rmi [ID]      # 이미지 삭제
```

---

## 요약: 재설치 시 핵심 변경 사항만 정리

| 항목 | 상태 | 비고 |
|------|------|------|
| Docker Desktop | 그대로 유지 | 재설치 불필요 |
| WSL 설정 | 유지됨 | 재활성화 필요 없음 |
| Ubuntu | 새로 설치 | 사용자 계정/환경 재구성 필요 |
| Docker CLI | Ubuntu에서 재연동 필요 | `docker-cli` 설치 또는 확인 필요 |
| WSL Integration | 다시 설정 필요 | Docker Desktop에서 새 Ubuntu 체크 |
| .wslconfig | 유지됨 | 별도 수정 불필요 |
| VS Code 연결 | 다시 연결 필요 | 새 Ubuntu 기준으로 연결 |

---

# 설치 대상 구성 요소

| 구성 요소     | 버전             | 설치 경로        |
|--------------|------------------|------------------|
| Apache Kafka | 3.3.0 (Scala 2.12) | `/usr/local/kafka` |
| Apache Spark | 3.5.4            | `/usr/local/spark` |
| Apache Flink | 1.20.0           | `/usr/local/flink` |
| PostgreSQL   | 16 + pgvector    | 시스템 기본 위치   |

---

# 설치 순서 요약

1. Java 11 설치
2. Python 가상환경 설정
3. Kafka 설치 및 Zookeeper 실행 확인
4. Spark 설치 및 실행 확인
5. Flink 설치 및 실행 확인
6. PostgreSQL 16 + pgvector 설치 및 계정 생성

---


## 사전 준비

```bash
sudo apt update
sudo apt install wget curl unzip git
```


### Java 11 설치

Kafka, Spark, Flink는 모두 Java 기반이므로 Java 11 JDK 설치가 필요합니다.

```bash
sudo apt update
sudo apt install -y openjdk-11-jdk
# ~/.bashrc에 명령어를 삽입해라
echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc

# ~/.bashrc에 새로운 명령어를 작성했으니 재시작해라
source ~/.bashrc
```

---

## Python 가상환경 구성 (앞으로 다룰 모든 python 패키지는 여기서 통일합니다.)

### Python 3.10 설치용 PPA 추가

```bash
sudo apt install -y software-properties-common
sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo apt update
```

### Python 3.10 및 venv 설치

```bash
sudo apt install -y python3.10 python3.10-venv python3.10-distutils
```

### 가상환경 생성 및 활성화

```bash
python3.10 -m venv ~/venvs/data-pjt
source ~/venvs/data-pjt/bin/activate
```

### Python 버전 확인

```bash
python --version  # Python 3.10.x

pip install -r requirements.txt
```

---

## WSL2 Ubuntu 환경에서 Kafka, Spark, Flink, PostgreSQL + pgvector 설치 가이드

## 1. Apache Kafka 3.3.0 (Scala 2.12) 설치

```bash
cd /usr/local
sudo wget https://dlcdn.apache.org/kafka/3.9.0/kafka_2.12-3.9.0.tgz
sudo tar -xzf kafka_2.12-3.9.0.tgz
sudo mv kafka_2.12-3.9.0 kafka
sudo rm kafka_2.12-3.9.0.tgz
```

---

## 2. Apache Spark 3.5.4 설치

```bash
cd /usr/local
sudo wget https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
sudo tar -xzf spark-3.5.4-bin-hadoop3.tgz
sudo mv spark-3.5.4-bin-hadoop3 spark
sudo rm spark-3.5.4-bin-hadoop3.tgz

```

**Spark 실행:**

```bash
cd /usr/local/spark
./bin/pyspark  
```

---

## 3. Apache Flink 1.20.0 설치

```bash
cd /usr/local
sudo wget https://downloads.apache.org/flink/flink-1.20.0/flink-1.20.0-bin-scala_2.12.tgz
sudo tar -xzf flink-1.20.0-bin-scala_2.12.tgz
sudo mv flink-1.20.0 flink
sudo rm flink-1.20.0-bin-scala_2.12.tgz
```

## 환경 변수 설정

`~/.bashrc` 파일에 다음을 추가
vi ~/.bashrc
```bash
export KAFKA_HOME=/usr/local/kafka
export SPARK_HOME=/usr/local/spark
export FLINK_HOME=/usr/local/flink
export PYFLINK_PYTHON=/home/ssafy/venvs/data-pjt/bin/python
export PATH=$PATH:$KAFKA_HOME/bin:$SPARK_HOME/bin:$FLINK_HOME/bin
```

변경 적용:

```bash
source ~/.bashrc
```

```bash
cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
chmod +x $SPARK_HOME/conf/spark-env.sh
```

### spark-env.sh 내에 추가 (클러스터 형으로 돌리기 위해서 필요한 설정) (강의에서는 필요없으나 webui는 airflow와 port 충돌 방지를 위해 설정)

```bash
export SPARK_MASTER_WEBUI_PORT=8082
export SPARK_MASTER_HOST=0.0.0.0
export SPARK_LOCAL_IP=0.0.0.0
```

--- 

**Kafka 실행 및 종료**

```bash
# 터미널 1
$KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties
^c 

# 터미널 2
$KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties
```


**Spark 실행**
standalone 클러스터 모드 사용
```bash
$SPARK_HOME/sbin/start-master.sh
$SPARK_HOME/sbin/start-worker.sh
jps
$SPARK_HOME/sbin/stop-master.sh
$SPARK_HOME/sbin/stop-worker.sh
```
**Flink 실행**

```bash
sudo chown -R ssafy:ssafy /usr/local

$FLINK_HOME/bin/start-cluster.sh
$FLINK_HOME/bin/stop-cluster.sh
```
$FLINK_HOME/bin/flink run -py skeleton.py

---

## 4. PostgreSQL 16 및 pgvector 설치

### PostgreSQL 설치

```bash
sudo apt-get update
sudo apt-get install postgresql-16 postgresql-contrib-16 postgresql-16-pgvector -y  
```

**서비스 상태 확인**  

```bash
sudo service postgresql status
```

1. **PostgreSQL 접속**  

```bash
sudo -i -u postgres
psql
```

2. **데이터베이스 생성**  

```sql
CREATE DATABASE news;
```

3. **사용자 생성 및 권한 부여**  

```sql
CREATE USER ssafyuser WITH PASSWORD 'ssafy';
GRANT ALL PRIVILEGES ON DATABASE news TO ssafyuser;
```

4. **접속 인증 설정 (pg_hba.conf 수정)**

sudo vi /etc/postgresql/16/main/pg_hba.conf

**아래와 같이 수정**
```conf
# Allow replication connections from localhost, by a user with the
# replication privilege.
local   all             all                                     md5
host    all             all             127.0.0.1/32            md5
host    all             all             ::1/128                 md5
```

5. **데이터베이스 접속 및 테이블 생성성**

** postgre 재시작 **
```bash
sudo systemctl restart postgresql
```

```sql
\c news
```

```sql
-- pgvector 확장 설치 (최초 1회)
CREATE EXTENSION IF NOT EXISTS vector;

-- news_article 테이블 생성
CREATE TABLE news_article (
    id SERIAL PRIMARY KEY,
    title VARCHAR(200) NOT NULL,
    writer VARCHAR(255) NOT NULL,
    write_date TIMESTAMP NOT NULL,
    category VARCHAR(50) NOT NULL,
    content TEXT NOT NULL,
    url VARCHAR(200) UNIQUE NOT NULL,
    keywords JSON DEFAULT '[]'::json,
    embedding VECTOR(1536) NOT NULL
);
```

6. **권한 부여**

```sql
ALTER DEFAULT PRIVILEGES IN SCHEMA public
GRANT ALL ON TABLES TO ssafyuser;

ALTER DEFAULT PRIVILEGES IN SCHEMA public
GRANT ALL ON SEQUENCES TO ssafyuser;
GRANT CREATE ON SCHEMA public TO ssafyuser;
```

---