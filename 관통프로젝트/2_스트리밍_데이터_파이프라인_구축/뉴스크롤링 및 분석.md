# á„‚á…²á„‰á…³ á„á…³á„…á…©á†¯á„…á…µá†¼ á„†á…µá†¾ á„‡á…®á†«á„‰á…¥á†¨

âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ â†’ í‚¤ì›Œë“œ/ì„ë² ë”© ë¶„ì„ â†’ PostgreSQL ì €ì¥ ìë™í™” ì½”ë“œ (ì „ì²´ ì£¼ì„ í¬í•¨ ì„¤ëª…)
==========================================================

```python
# í•„ìš”í•œ ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ import í•©ë‹ˆë‹¤.
import feedparser  # RSS í”¼ë“œë¥¼ ì½ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests  # ì›¹ ìš”ì²­ì„ ë³´ë‚´ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
import os  # OS í™˜ê²½ ì„¤ì • ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from bs4 import BeautifulSoup  # HTML ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import psycopg2  # PostgreSQLì— ì—°ê²°í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import json  # ë°ì´í„°ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime  # ë‚ ì§œì™€ ì‹œê°„ ê´€ë ¨ ê¸°ëŠ¥ì„ ìœ„í•œ ëª¨ë“ˆ
from dotenv import load_dotenv  # .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.feature_extraction.text import TfidfVectorizer  # í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ TF-IDF ë²¡í„°ë¼ì´ì €
from sentence_transformers import SentenceTransformer  # Ko-SBERT ëª¨ë¸ì„ í†µí•´ ì„ë² ë”© ì¶”ì¶œ

# .env íŒŒì¼ì— ìˆëŠ” í™˜ê²½ ë³€ìˆ˜(DB ID, PW ë“±)ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
load_dotenv()

# PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œë¶€í„° ì½ì–´ì˜µë‹ˆë‹¤.
DB_CONFIG = {
    "dbname": "news",  # ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„
    "user": os.getenv("DB_USERNAME"),  # .envì—ì„œ ë¶ˆëŸ¬ì˜¨ ì‚¬ìš©ìëª…
    "password": os.getenv("DB_PASSWORD"),  # ë¹„ë°€ë²ˆí˜¸
    "host": "localhost",  # í˜¸ìŠ¤íŠ¸ ì£¼ì†Œ (ë³´í†µ ë¡œì»¬ì´ë©´ localhost)
    "port": 5432  # PostgreSQL ê¸°ë³¸ í¬íŠ¸ ë²ˆí˜¸
}

# ìˆ˜ì§‘í•  RSS í”¼ë“œ ì£¼ì†Œ (ë§¤ì¼ê²½ì œ ì‚¬íšŒë©´ ê¸°ì‚¬)
RSS_FEED_URL = "https://www.mk.co.kr/rss/30100041/"

# ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë”© (Ko-SBERT)
embedding_model = SentenceTransformer("jhgan/ko-sbert-nli")

# ğŸ” ë‰´ìŠ¤ ë³¸ë¬¸ê³¼ ê¸°ì ì´ë¦„ì„ í•´ë‹¹ ê¸°ì‚¬ ë§í¬ì—ì„œ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜
def extract_article_text(url):
    try:
        # ê¸°ì‚¬ í˜ì´ì§€ë¥¼ ìš”ì²­
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.text, "html.parser")

        content = ""  # ë³¸ë¬¸ ì´ˆê¸°í™”

        # ì¼ë°˜ì ìœ¼ë¡œ ë‰´ìŠ¤ ë³¸ë¬¸ì´ ë“¤ì–´ìˆëŠ” div class ì„ íƒ
        wrapper = soup.find("div", class_="news_cnt_detail_wrap")
        if wrapper:
            paragraphs = wrapper.find_all("p")  # ì—¬ëŸ¬ <p> íƒœê·¸ë¡œ êµ¬ì„±ëœ ë³¸ë¬¸ ì¶”ì¶œ
            if paragraphs:
                # ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ë¶™ì¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸
                content = "\n\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        else:
            # ìœ„ ë°©ì‹ì´ ì•ˆ ë¨¹íˆëŠ” ì˜ˆì™¸ì ì¸ ê²½ìš° figcaptionë„ ì‹œë„
            figcaption = soup.select_one("figure figcaption")
            if figcaption:
                content = figcaption.get_text(strip=True)

        # ë³¸ë¬¸ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
        if not content:
            content = "[ë³¸ë¬¸ ì—†ìŒ]"

        # ê¸°ì ì´ë¦„ ì¶”ì¶œ (ì—¬ëŸ¬ ëª…ì¼ ìˆ˜ ìˆìŒ)
        author_tags = soup.select("dl.author > a")
        writers = [tag.get_text(strip=True) for tag in author_tags]
        writer = ", ".join(writers) if writers else "ì•Œ ìˆ˜ ì—†ìŒ"

        return content, writer

    except Exception as e:
        print(f"[ë³¸ë¬¸/ê¸°ì í¬ë¡¤ë§ ì˜¤ë¥˜] {url} â†’ {e}")
        return "[ë³¸ë¬¸ ì—†ìŒ]", "ì•Œ ìˆ˜ ì—†ìŒ"

# ğŸ§  TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_keywords(text, top_n=5):
    try:
        vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
        tfidf_matrix = vectorizer.fit_transform([text])  # ë¬¸ì„œ í•˜ë‚˜ë¡œ í•™ìŠµ
        feature_names = vectorizer.get_feature_names_out()  # ë‹¨ì–´ ëª©ë¡
        scores = tfidf_matrix.toarray()[0]  # ê° ë‹¨ì–´ì˜ TF-IDF ì ìˆ˜
        keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)[:top_n]  # ë†’ì€ ìˆœ
        return [word for word, _ in keywords]  # í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
    except Exception as e:
        print(f"[í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜] {e}")
        return []

# ğŸ§¬ ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•´ì£¼ëŠ” í•¨ìˆ˜
def generate_embedding(text):
    try:
        return embedding_model.encode(text).tolist()  # numpy â†’ listë¡œ ë³€í™˜
    except Exception as e:
        print(f"[ì„ë² ë”© ì˜¤ë¥˜] {e}")
        return None

# ğŸ—‚ í•˜ë‚˜ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ PostgreSQLì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_article_to_db(title, writer, pub_date, category, content, link):
    try:
        # DB ì—°ê²°
        conn = psycopg2.connect(**DB_CONFIG)
        cur = conn.cursor()

        # ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ (ì´ë¯¸ ê°™ì€ URLì´ ì €ì¥ë˜ì–´ ìˆìœ¼ë©´ íŒ¨ìŠ¤)
        cur.execute("SELECT 1 FROM news_article WHERE url = %s", (link,))
        if cur.fetchone():
            print(f"[ì¤‘ë³µ] ì´ë¯¸ ì¡´ì¬: {title}")
        else:
            # í‚¤ì›Œë“œì™€ ì„ë² ë”© ìƒì„±
            keywords = extract_keywords(content)
            embedding = generate_embedding(content)

            # ë°ì´í„° ì‚½ì…
            cur.execute(
                """
                INSERT INTO news_article 
                (title, writer, write_date, category, content, url, keywords, embedding)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                """,
                (
                    title,
                    writer,
                    pub_date,
                    category,
                    content,
                    link,
                    json.dumps(keywords, ensure_ascii=False),  # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    embedding  # KoSBERT ì„ë² ë”© ë²¡í„°
                )
            )
            conn.commit()
            print(f"[ì €ì¥ ì™„ë£Œ] {title}")

        # ì—°ê²° ì¢…ë£Œ
        cur.close()
        conn.close()

    except Exception as e:
        print(f"[DB ì €ì¥ ì˜¤ë¥˜] {title} â†’ {e}")

# ğŸ” ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ ë¶„ì„ â†’ ì €ì¥ ê³¼ì •ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
def main():
    print("ğŸ“° RSS í”¼ë“œ ìˆ˜ì§‘ ì‹œì‘")
    feed = feedparser.parse(RSS_FEED_URL)  # RSS í”¼ë“œ íŒŒì‹±

    for entry in feed.entries:
        title = entry.title  # ê¸°ì‚¬ ì œëª©
        link = entry.link  # ê¸°ì‚¬ ë§í¬
        pub_date = entry.get('published_parsed')
        pub_date = datetime(*pub_date[:6]) if pub_date else datetime.now()  # ì‘ì„±ì¼ì
        category = entry.get("category", "ê¸°íƒ€")  # ì¹´í…Œê³ ë¦¬ (ì—†ìœ¼ë©´ 'ê¸°íƒ€')

        # ê¸°ì‚¬ ë³¸ë¬¸ê³¼ ê¸°ìëª… ê°€ì ¸ì˜¤ê¸°
        content, writer = extract_article_text(link)
        if content and content != "[ë³¸ë¬¸ ì—†ìŒ]":
            save_article_to_db(title, writer, pub_date, category, content, link)

# ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í–ˆì„ ë•Œë§Œ main()ì„ ìˆ˜í–‰
if __name__ == "__main__":
    main()
```

* * *

ğŸ§  ì¶”ê°€ ê°œë… ì„¤ëª… ì •ë¦¬
--------------

| ê°œë… | ì„¤ëª… |
| --- | --- |
| **RSS í”¼ë“œ** | ë‰´ìŠ¤ë‚˜ ë¸”ë¡œê·¸ ë“±ì˜ ì—…ë°ì´íŠ¸ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ë°›ì•„ì˜¬ ìˆ˜ ìˆë„ë¡ ë§Œë“  XML í˜•ì‹ì˜ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ |
| **BeautifulSoup** | HTML êµ¬ì¡°ë¥¼ íŒŒì‹±í•´ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì‰½ê²Œ ì¶”ì¶œí•˜ê²Œ í•´ì£¼ëŠ” ë„êµ¬ |
| **TF-IDF** | í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ í†µê³„ì  ë°©ë²• (ë¹ˆë„ ê¸°ë°˜) |
| **Ko-SBERT** | í•œêµ­ì–´ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë°”ê¿”ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ ëª¨ë¸. ë¬¸ì¥ ê°„ ì˜ë¯¸ë¥¼ ë¹„êµ ê°€ëŠ¥ |
| **PostgreSQL** | ì˜¤í”ˆì†ŒìŠ¤ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ. ì—¬ê¸°ì„  ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš© |
| **.env íŒŒì¼** | ë¹„ë°€ë²ˆí˜¸ë‚˜ ê³„ì • ì •ë³´ ê°™ì€ ë¯¼ê°í•œ ê°’ì„ ë”°ë¡œ ë³´ê´€í•˜ëŠ” í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ |
